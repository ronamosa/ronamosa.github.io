"use strict";(self.webpackChunkronamosa_github_io=self.webpackChunkronamosa_github_io||[]).push([[23],{4740:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/AWSBedrockLangchainWK-LabT5-c15a5d04489cbc13bc0209c0396292d5.png"},14683:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/AWSBedrockLangchainWK-LabT2-3c59969b986a7dad45910807af688d7b.png"},28453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>o});var s=t(96540);const r={},a=s.createContext(r);function i(e){const n=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),s.createElement(a.Provider,{value:n},e.children)}},30093:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>i,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"engineer/AI/BedrockLangChainWorkshop1","title":"Amazon Bedrock and LangChain Workshop - Enterprise AI Application Development","description":"Complete workshop guide for building enterprise AI applications using Amazon Bedrock and LangChain. Learn serverless AI deployment and advanced LLM integration patterns.","source":"@site/docs/engineer/AI/BedrockLangChainWorkshop1.md","sourceDirName":"engineer/AI","slug":"/engineer/AI/BedrockLangChainWorkshop1","permalink":"/docs/engineer/AI/BedrockLangChainWorkshop1","draft":false,"unlisted":false,"editUrl":"https://github.com/ronamosa/ronamosa.github.io/edit/main/website/docs/engineer/AI/BedrockLangChainWorkshop1.md","tags":[{"inline":true,"label":"aws","permalink":"/docs/tags/aws"},{"inline":true,"label":"bedrock","permalink":"/docs/tags/bedrock"},{"inline":true,"label":"langchain","permalink":"/docs/tags/langchain"},{"inline":true,"label":"enterprise","permalink":"/docs/tags/enterprise"},{"inline":true,"label":"workshop","permalink":"/docs/tags/workshop"}],"version":"current","lastUpdatedBy":"Ron Amosa","lastUpdatedAt":1758526302000,"sidebarPosition":12,"frontMatter":{"title":"Amazon Bedrock and LangChain Workshop - Enterprise AI Application Development","description":"Complete workshop guide for building enterprise AI applications using Amazon Bedrock and LangChain. Learn serverless AI deployment and advanced LLM integration patterns.","keywords":["amazon bedrock","langchain","enterprise ai","serverless ai","aws bedrock","llm integration","ai applications","bedrock workshop"],"tags":["aws","bedrock","langchain","enterprise","workshop"],"sidebar_position":12},"sidebar":"docsSidebar","previous":{"title":"AI Prompting Collection - System, User, and Advanced Prompt Templates","permalink":"/docs/engineer/AI/PromptingAI"},"next":{"title":"LangChain AutoGPT Development - Building Autonomous AI Agents","permalink":"/docs/engineer/AI/LLMLangChainProject"}}');var r=t(74848),a=t(28453);const i={title:"Amazon Bedrock and LangChain Workshop - Enterprise AI Application Development",description:"Complete workshop guide for building enterprise AI applications using Amazon Bedrock and LangChain. Learn serverless AI deployment and advanced LLM integration patterns.",keywords:["amazon bedrock","langchain","enterprise ai","serverless ai","aws bedrock","llm integration","ai applications","bedrock workshop"],tags:["aws","bedrock","langchain","enterprise","workshop"],sidebar_position:12},o=void 0,l={},d=[{value:"Running in my own AWS account",id:"running-in-my-own-aws-account",level:2},{value:"Enable Bedrock",id:"enable-bedrock",level:3},{value:"AWS Cloud9 setup",id:"aws-cloud9-setup",level:3},{value:"Local Setup",id:"local-setup",level:3},{value:"Foundational Concepts",id:"foundational-concepts",level:2},{value:"API",id:"api",level:3},{value:"Langchain",id:"langchain",level:3},{value:"Inference Parameters",id:"inference-parameters",level:3},{value:"Control Response Variability",id:"control-response-variability",level:3},{value:"Streaming API",id:"streaming-api",level:3},{value:"Embeddings",id:"embeddings",level:3},{value:"Streamlit",id:"streamlit",level:3},{value:"Model Selection",id:"model-selection",level:3},{value:"Basic patterns",id:"basic-patterns",level:2},{value:"B1 Text Generation",id:"b1-text-generation",level:3},{value:"B2 Image Generation",id:"b2-image-generation",level:3},{value:"B3 RAG",id:"b3-rag",level:3},{value:"B4 Chatbot",id:"b4-chatbot",level:3},{value:"Text Patterns",id:"text-patterns",level:2},{value:"T1 Chatbot RAG",id:"t1-chatbot-rag",level:3},{value:"T2 Doc Summary",id:"t2-doc-summary",level:3},{value:"T3 Response Streaming",id:"t3-response-streaming",level:3},{value:"T4 Embeddings Search",id:"t4-embeddings-search",level:3},{value:"T5 Personalised Recommendations",id:"t5-personalised-recommendations",level:3},{value:"T6 Extract JSON",id:"t6-extract-json",level:3},{value:"T7 Text to CSV",id:"t7-text-to-csv",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Error Messages",id:"error-messages",level:3}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"header",src:t(59893).A+"",width:"2756",height:"1134"})}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsxs)(n.p,{children:["These are my notes for the ",(0,r.jsx)(n.a,{href:"https://catalog.workshops.aws/building-with-amazon-bedrock/en-US",children:"Workshop"})," Section."]})}),"\n",(0,r.jsx)(n.p,{children:"In the workshop you have two methods of running the labs, at an AWS event, or in your own account."}),"\n",(0,r.jsx)(n.h2,{id:"running-in-my-own-aws-account",children:"Running in my own AWS account"}),"\n",(0,r.jsx)(n.h3,{id:"enable-bedrock",children:"Enable Bedrock"}),"\n",(0,r.jsx)(n.p,{children:"I've already done this."}),"\n",(0,r.jsx)(n.h3,{id:"aws-cloud9-setup",children:"AWS Cloud9 setup"}),"\n",(0,r.jsxs)(n.p,{children:["spin up a ",(0,r.jsx)(n.code,{children:"t3.small"})," EC2 instance."]}),"\n",(0,r.jsx)(n.p,{children:"pull down the repo:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd ~/environment/\ncurl 'https://static.us-east-1.prod.workshops.aws/public/b41bacc3-e25c-4826-8554-b4aa2cb9a2e5/assets/workshop.zip' --output workshop.zip\nunzip workshop.zip\n"})}),"\n",(0,r.jsx)(n.p,{children:"install requirements"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip3 install -r ~/environment/workshop/setup/requirements.txt -U\n"})}),"\n",(0,r.jsx)(n.p,{children:"test working"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cloudbuilderio:~/environment/workshop $ python3 ./completed/api/bedrock_api.py \n\nManchester is the largest and most populous city in New Hampshire.\n"})}),"\n",(0,r.jsx)(n.h3,{id:"local-setup",children:"Local Setup"}),"\n",(0,r.jsx)(n.p,{children:"Please note, for a few of the labs I ran it in my local Linux environment which required specific setup to get things going."}),"\n",(0,r.jsxs)(n.p,{children:["I still downloaded the ",(0,r.jsx)(n.code,{children:"workshop.zip"})," and followed instructions as per, but had to tweak my environment along the way."]}),"\n",(0,r.jsxs)(n.p,{children:["A few things if you're going to run local, in the root ",(0,r.jsx)(n.code,{children:"workshop/"})," directory:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["create a virtual env: ",(0,r.jsx)(n.code,{children:"python3 -m venv .env"})]}),"\n",(0,r.jsxs)(n.li,{children:["activate it: ",(0,r.jsx)(n.code,{children:"source .env/bin/activate"})]}),"\n",(0,r.jsxs)(n.li,{children:["install dependencies ",(0,r.jsx)(n.code,{children:"pip3 install -r requirements"})]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"I will list my compiled requirements.txt here:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-text",children:"# requirements\nboto3\nlangchain_community\nstreamlit\nlangchain\npypdf\n"})}),"\n",(0,r.jsx)(n.h2,{id:"foundational-concepts",children:"Foundational Concepts"}),"\n",(0,r.jsx)(n.p,{children:"Play around with examples, play with temp, top p, response length."}),"\n",(0,r.jsx)(n.p,{children:"View API request doesn't show up on all examples (greyed out)."}),"\n",(0,r.jsx)(n.p,{children:"Here's one:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'aws bedrock-runtime invoke-model \\\n--model-id meta.llama2-13b-chat-v1 \\\n--body "{\\"prompt\\":\\"[INST]You are a a very intelligent bot with exceptional critical thinking[/INST]\\\\nI went to the market and bought 10 apples. I gave 2 apples to your friend and 2 to the helper. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\\\\n\\\\nLet\'s think step by step.\\\\n\\\\n\\\\nFirst, I went to the market and bought 10 apples.\\\\n\\\\nThen, I gave 2 apples to your friend.\\\\n\\\\nSo, I have 10 - 2 = 8 apples left.\\\\n\\\\nNext, I gave 2 apples to the helper.\\\\n\\\\nSo, I have 8 - 2 = 6 apples left.\\\\n\\\\nNow, I went and bought 5 more apples.\\\\n\\\\nSo, I have 6 + 5 = 11 apples left.\\\\n\\\\nFinally, I ate 1 apple.\\\\n\\\\nSo, I have 11 - 1 = 10 apples left.\\\\n\\\\nTherefore, I remain with 10 apples.\\",\\"max_gen_len\\":512,\\"temperature\\":0.5,\\"top_p\\":0.9}" \\\n--cli-binary-format raw-in-base64-out \\\n--region us-east-1 \\\ninvoke-model-output.txt\n'})}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsxs)(n.p,{children:["The API call was most familiar to me because of my SageMaker LLM project, but for that I pointed at an inference endpoint, whereas here we call the ",(0,r.jsx)(n.code,{children:"--model-id"}),"."]})}),"\n",(0,r.jsx)(n.h3,{id:"api",children:"API"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import json\nimport boto3\n\nsession = boto3.Session()\n\nbedrock = session.client(service_name=\'bedrock-runtime\') #creates a Bedrock client\n\nbedrock_model_id = "ai21.j2-ultra-v1" #set the foundation model\n\nprompt = "What\'s the name of the emerald mine that Elon Musk\'s father owns?" #the prompt to send to the model\n\nbody = json.dumps({\n    "prompt": prompt, #AI21\n    "maxTokens": 1024, \n    "temperature": 0, \n    "topP": 0.5, \n    "stopSequences": [], \n    "countPenalty": {"scale": 0 }, \n    "presencePenalty": {"scale": 0 }, \n    "frequencyPenalty": {"scale": 0 }\n}) #build the request payload\n\n# invoke\n\nresponse = bedrock.invoke_model(body=body, modelId=bedrock_model_id, accept=\'application/json\', contentType=\'application/json\') #send the payload to Bedrock\n\nresponse_body = json.loads(response.get(\'body\').read()) # read the response\n\nresponse_text = response_body.get("completions")[0].get("data").get("text") #extract the text from the JSON response\n\nprint(response_text)\n'})}),"\n",(0,r.jsx)(n.p,{children:"output"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"~/R/AWSB/w/l/api \u276f python3 ./bedrock_api.py\n\nElon Musk's father, Errol Musk, owns the emerald mine in Chivor, Colombia.\n"})}),"\n",(0,r.jsxs)(n.p,{children:["I originally set my prompt to ",(0,r.jsx)(n.code,{children:'"Write a poem about Serena Williams"'})," and this is what I got:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-text",children:"~/R/AWSB/w/l/api \u276f python3 ./bedrock_api.py             took 4s\n\nManchester is the largest and most populous city in New Hampshire.\n\n~/R/AWSB/w/l/api \u276f python3 ./bedrock_api.py            took 19s\n\nSerena Williams,\n\nA champion on the court,\n\nA role model off,\n\nA fierce competitor,\n\nA fierce advocate for equality,\n\nA fierce advocate for women's rights,\n\nA fierce advocate for social justice,\n\nA fierce advocate for change,\n\nA fierce advocate for herself,\n\nA fierce advocate for others,\n\nA fierce advocate for the game,\n\nA fierce advocate for the sport,\n\nA fierce advocate for the world,\n\nA fierce advocate for humanity,\n\nA fierce advocate for love,\n\nA fierce advocate for life,\n\nA fierce advocate for everything,\n\nA fierce advocate for nothing,\n\nA fierce advocate for everything,\n\nA fierce advocate for nothing,\n...\n\n# repeats the everything, nothing line again 263 times!!!\n"})}),"\n",(0,r.jsx)(n.p,{children:"a bit \ud83d\ude2c."}),"\n",(0,r.jsxs)(n.admonition,{title:"Speed",type:"tip",children:[(0,r.jsx)(n.p,{children:"\u2705 For the single answer questions, the API is really quite fast: ~4s"}),(0,r.jsx)(n.p,{children:"\u26a0\ufe0f The poem took a while ~19s but from the output, looked caught in a loop."})]}),"\n",(0,r.jsx)(n.h3,{id:"langchain",children:"Langchain"}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{}),(0,r.jsx)(n.th,{children:"\u2705 Pros"}),(0,r.jsx)(n.th,{children:"\u274c Cons"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"boto3"}),(0,r.jsx)(n.td,{children:"more control, details"}),(0,r.jsx)(n.td,{children:"have to handle, manage more details"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Langchain"}),(0,r.jsx)(n.td,{children:"abstracted, focus on text in and out"}),(0,r.jsx)(n.td,{children:"less verbose, granular than boto3"})]})]})]})}),"\n",(0,r.jsx)(n.p,{children:"Code:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="bedrock_langchain.py" showLineNumbers',children:'from langchain_community.llms import Bedrock\n\nllm = Bedrock( #create a Bedrock llm client\n    model_id="ai21.j2-ultra-v1" #set the foundation model\n)\n\nprompt = "What is the largest city in New Zealand?"\n\nresponse_text = llm.invoke(prompt) #return a response to the prompt\n\nprint(response_text)\n'})}),"\n",(0,r.jsx)(n.p,{children:"output"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"~/R/AWSB/w/l/langchain \u276f python3 ./bedrock_langchain.py \n\nThe largest city in New Zealand is Auckland, with a population of approximately 1.5 million. It is located\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Code must smaller than with ",(0,r.jsx)(n.code,{children:"boto3"}),"."]}),"\n",(0,r.jsx)(n.h3,{id:"inference-parameters",children:"Inference Parameters"}),"\n",(0,r.jsx)(n.admonition,{title:"missing updates.",type:"caution",children:(0,r.jsxs)(n.p,{children:["I had to update some details in the workshop code as default params for the models had been updated e.g. for Anthropic, the parameter is replaced ",(0,r.jsx)(n.code,{children:"max_tokens"})," with ",(0,r.jsx)(n.code,{children:"max_tokens_to_sample"})]})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="params.py" showLineNumbers',children:'import sys\nfrom langchain_community.llms import Bedrock\n\ndef get_inference_parameters(model): #return a default set of parameters based on the model\'s provider\n    bedrock_model_provider = model.split(\'.\')[0] #grab the model provider from the first part of the model id\n    \n    if (bedrock_model_provider == \'anthropic\'): #Anthropic model\n        return { #anthropic\n            "max_tokens_to_sample": 512, # my update\n            "temperature": 0, \n            "top_k": 250, \n            "top_p": 1, \n            "stop_sequences": ["\\n\\nHuman:"] \n           }\n    \n    elif (bedrock_model_provider == \'ai21\'): #AI21\n        return { #AI21\n            "maxTokens": 512, \n            "temperature": 0, \n            "topP": 0.5, \n            "stopSequences": [], \n            "countPenalty": {"scale": 0 }, \n            "presencePenalty": {"scale": 0 }, \n            "frequencyPenalty": {"scale": 0 } \n           }\n    \n    elif (bedrock_model_provider == \'cohere\'): #COHERE\n        return {\n            "max_tokens": 512,\n            "temperature": 0,\n            "p": 0.01,\n            "k": 0,\n            "stop_sequences": [],\n            "return_likelihoods": "NONE"\n        }\n    \n    elif (bedrock_model_provider == \'meta\'): #META\n        return {\n            "temperature": 0,\n            "top_p": 0.9,\n            "max_gen_len": 512\n        }\n    \n    elif (bedrock_model_provider == \'mistral\'): #MISTRAL\n        return {\n            "max_tokens" : 512,\n            "stop" : [],    \n            "temperature": 0,\n            "top_p": 0.9,\n            "top_k": 50\n        } \n\n    else: #Amazon\n        #For the LangChain Bedrock implementation, these parameters will be added to the \n        #textGenerationConfig item that LangChain creates for us\n        return { \n            "maxTokenCount": 512, \n            "stopSequences": [], \n            "temperature": 0, \n            "topP": 0.9 \n        }\n\n# setup a function that pulls our request params together\ndef get_text_response(model, input_content): #text-to-text client function\n    \n    model_kwargs = get_inference_parameters(model) #get the default parameters based on the selected model\n    \n    llm = Bedrock( #create a Bedrock llm client\n        model_id=model, #use the requested model\n        model_kwargs = model_kwargs\n    )\n    \n    return llm.invoke(input_content) #return a response to the prompt\n\n# make a call, capture in response\nresponse = get_text_response(sys.argv[1], sys.argv[2])\n\nprint(response)\n'})}),"\n",(0,r.jsxs)(n.p,{children:["Run it with args (cos you asked for ",(0,r.jsx)(n.code,{children:"sys.argv[1]"})," and ",(0,r.jsx)(n.code,{children:"sys.argv[2]"}),"):"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:'python3 ./params.py "ai21.j2-ultra-v1" "Write a haiku:"'})}),"\n",(0,r.jsx)(n.p,{children:"output:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'~/R/AWSB/w/l/params \u276f python3 ./params.py "ai21.j2-ultra-v1" "Write a haiku:"\n\nleaves rustle in breeze\nautumn colors slowly fade\nnature\'s symphony\n'})}),"\n",(0,r.jsx)(n.h3,{id:"control-response-variability",children:"Control Response Variability"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="temperature.py" showLineNumbers',children:'import sys\nfrom langchain_community.llms import Bedrock\n\ndef get_text_response(input_content, temperature): #text-to-text client function\n  \n  model_kwargs = { #AI21\n      "maxTokens": 1024, \n      "temperature": temperature, \n      "topP": 0.5, \n      "stopSequences": [], \n      "countPenalty": {"scale": 0 }, \n      "presencePenalty": {"scale": 0 }, \n      "frequencyPenalty": {"scale": 0 } \n  }\n  \n  llm = Bedrock( #create a Bedrock llm client\n      model_id="ai21.j2-ultra-v1",\n      model_kwargs = model_kwargs\n  )\n  \n  return llm.invoke(input_content) #return a response to the prompt\n\nfor i in range(3):\n  response = get_text_response(sys.argv[1], float(sys.argv[2]))\n  print(response)\n'})}),"\n",(0,r.jsxs)(n.p,{children:["Basically, you're setting up the function to take ",(0,r.jsx)(n.code,{children:"temperature"})," argument from user, pass it into the model kwargs."]}),"\n",(0,r.jsxs)(n.p,{children:["A temperature of ",(0,r.jsx)(n.code,{children:"0.0"})," should give you same reponse every time, anything over that should have some variety."]}),"\n",(0,r.jsx)(n.p,{children:"output:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'/workshop/labs/temperature \u276f python3 ./temperature.py "Write a haiku about China" 1.0s\n\nChina - vast and ancient\nA land of contrasts and wonders\nA tapestry woven\n\nChina - vast and ancient\nA land of contrasts and mystery\nA tapestry woven through time\n\nChina - vast and ancient\nA land of contrasts and wonders\nA tapestry woven\n/workshop/labs/temperature \u276f python3 ./temperature.py "Write a haiku about China" 1.0s\n\nChina - vast and ancient\nA land of contrasts and mystery\nA tapestry woven through time\n\nChina - vast and ancient\nA land of contrasts and wonders\nA journey to discovery\n\nChina - vast and ancient\nA land of contrasts and wonders\nA tapestry woven\n/workshop/labs/temperature \u276f python3 ./temperature.py "Write a haiku about China" 1.0s\n\nChina - vast and ancient\nA land of contrasts and wonders\nA place to discover\n\nChina - vast and ancient\nA land of contrasts and wonders\nA journey to discovery\n\nChina - vast and ancient\nA land of contrasts and mystery\nA tapestry woven through time\n/workshop/labs/temperature \u276f python3 ./temperature.py "Write a haiku about China" 1.0s\n\nChina - vast and ancient\nA land of contrasts and mystery\nA fascinating country\n\nChina - vast and ancient\nA land of contrasts and wonders\nA journey to discovery\n\nChina - vast and ancient\nA land of contrasts and mystery\nA tapestry woven through time\n/workshop/labs/temperature \u276f python3 ./temperature.py "Write a haiku about China" 1.0s\n\nChina - vast and ancient\nA land of contrasts and mystery\nA tapestry woven through time\n\nChina - vast and ancient\nA land of contrasts and wonders\nA culture rich and beautiful\n\nChina - vast and ancient\nA land of contrasts and mystery\nA world of wonder\n'})}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsx)(n.p,{children:"Pretty shit tbh \ud83e\udd23"})}),"\n",(0,r.jsx)(n.h3,{id:"streaming-api",children:"Streaming API"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="streaming.py" showLineNumbers',children:'import json\nimport boto3\n\nsession = boto3.Session()\n\nbedrock = session.client(service_name=\'bedrock-runtime\') #creates a Bedrock client\n\ndef chunk_handler(chunk):\n  print(chunk, end=\'\')\n\ndef get_streaming_response(prompt, streaming_callback):\n\n  bedrock_model_id = "anthropic.claude-3-sonnet-20240229-v1:0" #set the foundation model\n\n  body = json.dumps({\n    "prompt": prompt, #ANTHROPIC\n    "max_tokens": 4000,\n    "temperature": 0, \n    "top_k": 250, \n    "top_p": 1, \n    "stop_sequences": ["\\n\\nHuman:"] \n})\n  \n  \n  \n  body = json.dumps({\n    "anthropic_version": "bedrock-2023-05-31",\n    "max_tokens": 8000,\n    "temperature": 0,\n    "messages": [\n      {\n        "role": "user",\n        "content": [{ "type": "text", "text": prompt } ]\n      }\n    ],\n  })\n  \n  response = bedrock.invoke_model_with_response_stream(modelId=bedrock_model_id, body=body) #invoke the streaming method\n  \n  for event in response.get(\'body\'):\n    chunk = json.loads(event[\'chunk\'][\'bytes\'])\n\n    if chunk[\'type\'] == \'content_block_delta\':\n      if chunk[\'delta\'][\'type\'] == \'text_delta\':\n        streaming_callback(chunk[\'delta\'][\'text\'])\n\nprompt = "Tell me a story about two puppies and two kittens who became best friends:"\n\nget_streaming_response(prompt, chunk_handler)\n'})}),"\n",(0,r.jsx)(n.p,{children:"Clunky, but works as expected:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"workshop/labs/intro_streaming \u276f python3 ./intro_streaming.py                                                                                                            took \uf252 10s \ue73c .env at \uf017 12:42:30\nHere is a story about two puppies and two kittens who became best friends:\n\nDaisy and Buddy were two rambunctious golden retriever puppies who loved to play and get into mischief. One sunny day, they dug their way under the fence into the neighbor's yard. To their surprise, they came face to face with two tiny kittens named Smokey and Ginger who had been born just a few weeks earlier. \n\nAt first, the puppies and kittens were wary of each other, having never seen animals like that before. Daisy barked and Buddy wagged his tail furiously. Smokey arched his back and hissed while little Ginger tried to hide behind a potted plant. But after circling each other cautiously, Daisy plopped down and let out a friendly puppy whine. Smokey was the first to relax, sniffing at the puppies' faces.\n\nFrom that day on, the four became an inseparable crew. The puppies were infinitely gentle and patient, letting the kittens climb all over them. They taught the kittens to play chase and tug-of-war with old socks. The kittens showed the puppies how to stalk and pounce on toys. They napped together in warm puppy piles, taking turns grooming each other's fur.\n\nAs they grew older, their differences didn't matter at all. Daisy, Buddy, Smokey and Ginger were the best of friends who loved romping in the yard, going on walks together, and curling up side-by-side at naptime and bedtime. Their unique little family brought joy to all the neighbors who watched their silly antics and special bond. The four friends proved that differences don't matter when you have fun, caring companions to share your days with.%\n"})}),"\n",(0,r.jsx)(n.h3,{id:"embeddings",children:"Embeddings"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="streaming.py" showLineNumbers',children:'from langchain_community.embeddings import BedrockEmbeddings\nfrom numpy import dot\nfrom numpy.linalg import norm\n\n#create an Amazon Titan Embeddings client\nbelc = BedrockEmbeddings()\n\nclass EmbedItem:\n  def __init__(self, text):\n    self.text = text\n    self.embedding = belc.embed_query(text)\n\nclass ComparisonResult:\n  def __init__(self, text, similarity):\n    self.text = text\n    self.similarity = similarity\n\ndef calculate_similarity(a, b): #See Cosine Similarity: https://en.wikipedia.org/wiki/Cosine_similarity\n  return dot(a, b) / (norm(a) * norm(b))\n\n#Build the list of embeddings to compare\nitems = []\n\nwith open("items.txt", "r") as f:\n  text_items = f.read().splitlines()\n\nfor text in text_items:\n  items.append(EmbedItem(text))\n\n# compare\nfor e1 in items:\n  print(f"Closest matches for \'{e1.text}\'")\n  print ("----------------")\n  cosine_comparisons = []\n  \n  for e2 in items:\n    similarity_score = calculate_similarity(e1.embedding, e2.embedding)\n    \n    cosine_comparisons.append(ComparisonResult(e2.text, similarity_score)) #save the comparisons to a list\n      \n  cosine_comparisons.sort(key=lambda x: x.similarity, reverse=True) # list the closest matches first\n  \n  for c in cosine_comparisons:\n    print("%.6f" % c.similarity, "\\t", c.text)\n  \n  print()\n'})}),"\n",(0,r.jsx)(n.p,{children:"output looks good, ranks match scores accordingly:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"python3 ./bedrock_embedding.py                                       took \uf252 31s \ue73c .env at \uf017 13:36:19\nClosest matches for 'Felines, canines, and rodents'\n----------------\n1.000000   Felines, canines, and rodents\n0.872856   Cats, dogs, and mice\n0.599730   Chats, chiens et souris\n0.516598   Lions, tigers, and bears\n0.455923   \u732b\u3001\u72ac\u3001\u30cd\u30ba\u30df\n0.068916   \u30d1\u30f3\u5c4b\u3078\u306e\u9053\u9806\u3092\u77e5\u308a\u305f\u3044\n0.061314   \u30d1\u30f3\u5c4b\u3078\u306e\u884c\u304d\u65b9\u3092\u6559\u3048\u3066\u304f\u3060\u3055\u3044\n0.002239   Can you please tell me how to get to the stadium?\n-0.003159   Kannst du mir bitte sagen, wie ich zur B\xe4ckerei komme?\n-0.007595   Can you please tell me how to get to the bakery?\n-0.019469   Pouvez-vous s'il vous pla\xeet me dire comment me rendre \xe0 la boulangerie?\n-0.020840   I need directions to the bread shop\n\nClosest matches for 'Can you please tell me how to get to the bakery?'\n----------------\n1.000000   Can you please tell me how to get to the bakery?\n0.712236   I need directions to the bread shop\n0.541959   Pouvez-vous s'il vous pla\xeet me dire comment me rendre \xe0 la boulangerie?\n0.484672   Can you please tell me how to get to the stadium?\n0.455479   \u30d1\u30f3\u5c4b\u3078\u306e\u884c\u304d\u65b9\u3092\u6559\u3048\u3066\u304f\u3060\u3055\u3044\n0.406388   \u30d1\u30f3\u5c4b\u3078\u306e\u9053\u9806\u3092\u77e5\u308a\u305f\u3044\n0.369163   Kannst du mir bitte sagen, wie ich zur B\xe4ckerei komme?\n0.078357   \u732b\u3001\u72ac\u3001\u30cd\u30ba\u30df\n0.022138   Cats, dogs, and mice\n0.015661   Lions, tigers, and bears\n0.005211   Chats, chiens et souris\n-0.007595   Felines, canines, and rodents\n\nClosest matches for 'Lions, tigers, and bears'\n----------------\n1.000000   Lions, tigers, and bears\n0.530917   Cats, dogs, and mice\n0.516598   Felines, canines, and rodents\n0.386125   Chats, chiens et souris\n0.337012   \u732b\u3001\u72ac\u3001\u30cd\u30ba\u30df\n0.068164   I need directions to the bread shop\n0.056721   Pouvez-vous s'il vous pla\xeet me dire comment me rendre \xe0 la boulangerie?\n0.054695   Kannst du mir bitte sagen, wie ich zur B\xe4ckerei komme?\n0.042972   \u30d1\u30f3\u5c4b\u3078\u306e\u9053\u9806\u3092\u77e5\u308a\u305f\u3044\n0.032731   Can you please tell me how to get to the stadium?\n0.021517   \u30d1\u30f3\u5c4b\u3078\u306e\u884c\u304d\u65b9\u3092\u6559\u3048\u3066\u304f\u3060\u3055\u3044\n0.015661   Can you please tell me how to get to the bakery?\n\nClosest matches for 'Chats, chiens et souris'\n----------------\n1.000000   Chats, chiens et souris\n0.669460   Cats, dogs, and mice\n0.599730   Felines, canines, and rodents\n0.498394   \u732b\u3001\u72ac\u3001\u30cd\u30ba\u30df\n0.386125   Lions, tigers, and bears\n0.299799   Pouvez-vous s'il vous pla\xeet me dire comment me rendre \xe0 la boulangerie?\n0.156950   \u30d1\u30f3\u5c4b\u3078\u306e\u9053\u9806\u3092\u77e5\u308a\u305f\u3044\n0.131597   \u30d1\u30f3\u5c4b\u3078\u306e\u884c\u304d\u65b9\u3092\u6559\u3048\u3066\u304f\u3060\u3055\u3044\n0.091534   Kannst du mir bitte sagen, wie ich zur B\xe4ckerei komme?\n0.025773   I need directions to the bread shop\n0.005211   Can you please tell me how to get to the bakery?\n-0.036810   Can you please tell me how to get to the stadium?\n\nClosest matches for '\u732b\u3001\u72ac\u3001\u30cd\u30ba\u30df'\n----------------\n1.000000   \u732b\u3001\u72ac\u3001\u30cd\u30ba\u30df\n0.503620   Cats, dogs, and mice\n0.498394   Chats, chiens et souris\n0.487732   \u30d1\u30f3\u5c4b\u3078\u306e\u9053\u9806\u3092\u77e5\u308a\u305f\u3044\n0.460217   \u30d1\u30f3\u5c4b\u3078\u306e\u884c\u304d\u65b9\u3092\u6559\u3048\u3066\u304f\u3060\u3055\u3044\n0.455923   Felines, canines, and rodents\n0.337012   Lions, tigers, and bears\n0.162600   Kannst du mir bitte sagen, wie ich zur B\xe4ckerei komme?\n0.153400   Pouvez-vous s'il vous pla\xeet me dire comment me rendre \xe0 la boulangerie?\n0.078357   Can you please tell me how to get to the bakery?\n0.063395   I need directions to the bread shop\n0.014240   Can you please tell me how to get to the stadium?\n\nClosest matches for 'Pouvez-vous s'il vous pla\xeet me dire comment me rendre \xe0 la boulangerie?'\n----------------\n1.000000   Pouvez-vous s'il vous pla\xeet me dire comment me rendre \xe0 la boulangerie?\n0.592948   I need directions to the bread shop\n0.541959   Can you please tell me how to get to the bakery?\n0.530933   Kannst du mir bitte sagen, wie ich zur B\xe4ckerei komme?\n0.433526   \u30d1\u30f3\u5c4b\u3078\u306e\u884c\u304d\u65b9\u3092\u6559\u3048\u3066\u304f\u3060\u3055\u3044\n0.383732   \u30d1\u30f3\u5c4b\u3078\u306e\u9053\u9806\u3092\u77e5\u308a\u305f\u3044\n0.299799   Chats, chiens et souris\n0.241092   Can you please tell me how to get to the stadium?\n0.153400   \u732b\u3001\u72ac\u3001\u30cd\u30ba\u30df\n0.056721   Lions, tigers, and bears\n0.031843   Cats, dogs, and mice\n-0.019469   Felines, canines, and rodents\n\nClosest matches for 'Kannst du mir bitte sagen, wie ich zur B\xe4ckerei komme?'\n----------------\n1.000000   Kannst du mir bitte sagen, wie ich zur B\xe4ckerei komme?\n0.530933   Pouvez-vous s'il vous pla\xeet me dire comment me rendre \xe0 la boulangerie?\n0.419582   I need directions to the bread shop\n0.369163   Can you please tell me how to get to the bakery?\n0.360738   \u30d1\u30f3\u5c4b\u3078\u306e\u884c\u304d\u65b9\u3092\u6559\u3048\u3066\u304f\u3060\u3055\u3044\n0.307116   \u30d1\u30f3\u5c4b\u3078\u306e\u9053\u9806\u3092\u77e5\u308a\u305f\u3044\n0.270668   Can you please tell me how to get to the stadium?\n0.162600   \u732b\u3001\u72ac\u3001\u30cd\u30ba\u30df\n0.091534   Chats, chiens et souris\n0.054695   Lions, tigers, and bears\n0.028943   Cats, dogs, and mice\n-0.003159   Felines, canines, and rodents\n\nClosest matches for '\u30d1\u30f3\u5c4b\u3078\u306e\u884c\u304d\u65b9\u3092\u6559\u3048\u3066\u304f\u3060\u3055\u3044'\n----------------\n1.000000   \u30d1\u30f3\u5c4b\u3078\u306e\u884c\u304d\u65b9\u3092\u6559\u3048\u3066\u304f\u3060\u3055\u3044\n0.895563   \u30d1\u30f3\u5c4b\u3078\u306e\u9053\u9806\u3092\u77e5\u308a\u305f\u3044\n0.491218   I need directions to the bread shop\n0.460217   \u732b\u3001\u72ac\u3001\u30cd\u30ba\u30df\n0.455479   Can you please tell me how to get to the bakery?\n0.433526   Pouvez-vous s'il vous pla\xeet me dire comment me rendre \xe0 la boulangerie?\n0.360738   Kannst du mir bitte sagen, wie ich zur B\xe4ckerei komme?\n0.220985   Can you please tell me how to get to the stadium?\n0.131597   Chats, chiens et souris\n0.078212   Cats, dogs, and mice\n0.061314   Felines, canines, and rodents\n0.021517   Lions, tigers, and bears\n\nClosest matches for '\u30d1\u30f3\u5c4b\u3078\u306e\u9053\u9806\u3092\u77e5\u308a\u305f\u3044'\n----------------\n1.000000   \u30d1\u30f3\u5c4b\u3078\u306e\u9053\u9806\u3092\u77e5\u308a\u305f\u3044\n0.895563   \u30d1\u30f3\u5c4b\u3078\u306e\u884c\u304d\u65b9\u3092\u6559\u3048\u3066\u304f\u3060\u3055\u3044\n0.487732   \u732b\u3001\u72ac\u3001\u30cd\u30ba\u30df\n0.466405   I need directions to the bread shop\n0.406388   Can you please tell me how to get to the bakery?\n0.383732   Pouvez-vous s'il vous pla\xeet me dire comment me rendre \xe0 la boulangerie?\n0.307116   Kannst du mir bitte sagen, wie ich zur B\xe4ckerei komme?\n0.156950   Chats, chiens et souris\n0.131994   Can you please tell me how to get to the stadium?\n0.101027   Cats, dogs, and mice\n0.068916   Felines, canines, and rodents\n0.042972   Lions, tigers, and bears\n\nClosest matches for 'Can you please tell me how to get to the stadium?'\n----------------\n1.000000   Can you please tell me how to get to the stadium?\n0.484672   Can you please tell me how to get to the bakery?\n0.305550   I need directions to the bread shop\n0.270668   Kannst du mir bitte sagen, wie ich zur B\xe4ckerei komme?\n0.241092   Pouvez-vous s'il vous pla\xeet me dire comment me rendre \xe0 la boulangerie?\n0.220985   \u30d1\u30f3\u5c4b\u3078\u306e\u884c\u304d\u65b9\u3092\u6559\u3048\u3066\u304f\u3060\u3055\u3044\n0.131994   \u30d1\u30f3\u5c4b\u3078\u306e\u9053\u9806\u3092\u77e5\u308a\u305f\u3044\n0.032731   Lions, tigers, and bears\n0.014240   \u732b\u3001\u72ac\u3001\u30cd\u30ba\u30df\n0.002239   Felines, canines, and rodents\n-0.008508   Cats, dogs, and mice\n-0.036810   Chats, chiens et souris\n\nClosest matches for 'I need directions to the bread shop'\n----------------\n1.000000   I need directions to the bread shop\n0.712236   Can you please tell me how to get to the bakery?\n0.592948   Pouvez-vous s'il vous pla\xeet me dire comment me rendre \xe0 la boulangerie?\n0.491218   \u30d1\u30f3\u5c4b\u3078\u306e\u884c\u304d\u65b9\u3092\u6559\u3048\u3066\u304f\u3060\u3055\u3044\n0.466405   \u30d1\u30f3\u5c4b\u3078\u306e\u9053\u9806\u3092\u77e5\u308a\u305f\u3044\n0.419582   Kannst du mir bitte sagen, wie ich zur B\xe4ckerei komme?\n0.305550   Can you please tell me how to get to the stadium?\n0.068164   Lions, tigers, and bears\n0.063395   \u732b\u3001\u72ac\u3001\u30cd\u30ba\u30df\n0.025934   Cats, dogs, and mice\n0.025773   Chats, chiens et souris\n-0.020840   Felines, canines, and rodents\n\nClosest matches for 'Cats, dogs, and mice'\n----------------\n1.000000   Cats, dogs, and mice\n0.872856   Felines, canines, and rodents\n0.669460   Chats, chiens et souris\n0.530917   Lions, tigers, and bears\n0.503620   \u732b\u3001\u72ac\u3001\u30cd\u30ba\u30df\n0.101027   \u30d1\u30f3\u5c4b\u3078\u306e\u9053\u9806\u3092\u77e5\u308a\u305f\u3044\n0.078212   \u30d1\u30f3\u5c4b\u3078\u306e\u884c\u304d\u65b9\u3092\u6559\u3048\u3066\u304f\u3060\u3055\u3044\n0.031843   Pouvez-vous s'il vous pla\xeet me dire comment me rendre \xe0 la boulangerie?\n0.028943   Kannst du mir bitte sagen, wie ich zur B\xe4ckerei komme?\n0.025934   I need directions to the bread shop\n0.022138   Can you please tell me how to get to the bakery?\n-0.008508   Can you please tell me how to get to the stadium?\n"})}),"\n",(0,r.jsx)(n.h3,{id:"streamlit",children:"Streamlit"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="streamlit.py" showLineNumbers',children:'#all streamlit commands will be available through the "st" alias\nimport streamlit as st\n\nst.set_page_config(page_title="\ud83d\udd17\ud83e\udd9c Streamlit Demo") #HTML title\nst.title("Streamlit Demo") #page title\n\ncolor_text = st.text_input("What\'s your favorite color?") #display a text box\ngo_button = st.button("Go", type="primary") #display a primary button\n\nif go_button:\n  #code in this if block will be run when the button is clicked\n    st.write(f"I like {color_text} too!") #display the response content\n\n'})}),"\n",(0,r.jsxs)(n.p,{children:["run it with streamlit's command; ",(0,r.jsx)(n.code,{children:"streamlit run simple_streamlit_app.py --server.port 8080"})]}),"\n",(0,r.jsx)(n.h3,{id:"model-selection",children:"Model Selection"}),"\n",(0,r.jsx)(n.p,{children:"No hard and fast rules about which model is best for given scenarios, all the ones available on Bedrock seem to do the same-ish thing. Each model will have relative strengths and weaknesses based on its training data, overall size, and training approach."}),"\n",(0,r.jsx)(n.admonition,{title:"Current Models",type:"info",children:(0,r.jsxs)(n.p,{children:["As as ",(0,r.jsx)(n.code,{children:"April 6, 2024"})]})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Provider"}),(0,r.jsx)(n.th,{children:"Model name"}),(0,r.jsx)(n.th,{children:"Version"}),(0,r.jsx)(n.th,{children:"Model ID"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Amazon"}),(0,r.jsx)(n.td,{children:"Titan Text G1 - Express"}),(0,r.jsx)(n.td,{children:"1.x"}),(0,r.jsx)(n.td,{children:"amazon.titan-text-express-v1"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Amazon"}),(0,r.jsx)(n.td,{children:"Titan Text G1 - Lite"}),(0,r.jsx)(n.td,{children:"1.x"}),(0,r.jsx)(n.td,{children:"amazon.titan-text-lite-v1"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Amazon"}),(0,r.jsx)(n.td,{children:"Titan Embeddings G1 - Text"}),(0,r.jsx)(n.td,{children:"1.x"}),(0,r.jsx)(n.td,{children:"amazon.titan-embed-text-v1"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Amazon"}),(0,r.jsx)(n.td,{children:"Titan Multimodal Embeddings G1"}),(0,r.jsx)(n.td,{children:"1.x"}),(0,r.jsx)(n.td,{children:"amazon.titan-embed-image-v1"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Amazon"}),(0,r.jsx)(n.td,{children:"Titan Image Generator G1"}),(0,r.jsx)(n.td,{children:"1.x"}),(0,r.jsx)(n.td,{children:"amazon.titan-image-generator-v1"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Anthropic"}),(0,r.jsx)(n.td,{children:"Claude"}),(0,r.jsx)(n.td,{children:"2.0"}),(0,r.jsx)(n.td,{children:"anthropic.claude-v2"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Anthropic"}),(0,r.jsx)(n.td,{children:"Claude"}),(0,r.jsx)(n.td,{children:"2.1"}),(0,r.jsx)(n.td,{children:"anthropic.claude-v2:1"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Anthropic"}),(0,r.jsx)(n.td,{children:"Claude 3 Sonnet"}),(0,r.jsx)(n.td,{children:"1.0"}),(0,r.jsx)(n.td,{children:"anthropic.claude-3-sonnet-20240229-v1:0"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Anthropic"}),(0,r.jsx)(n.td,{children:"Claude 3 Haiku"}),(0,r.jsx)(n.td,{children:"1.0"}),(0,r.jsx)(n.td,{children:"anthropic.claude-3-haiku-20240307-v1:0"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Anthropic"}),(0,r.jsx)(n.td,{children:"Claude Instant"}),(0,r.jsx)(n.td,{children:"1.x"}),(0,r.jsx)(n.td,{children:"anthropic.claude-instant-v1"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"AI21 Labs"}),(0,r.jsx)(n.td,{children:"Jurassic-2 Mid"}),(0,r.jsx)(n.td,{children:"1.x"}),(0,r.jsx)(n.td,{children:"ai21.j2-mid-v1"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"AI21 Labs"}),(0,r.jsx)(n.td,{children:"Jurassic-2 Ultra"}),(0,r.jsx)(n.td,{children:"1.x"}),(0,r.jsx)(n.td,{children:"ai21.j2-ultra-v1"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Cohere"}),(0,r.jsx)(n.td,{children:"Command"}),(0,r.jsx)(n.td,{children:"14.x"}),(0,r.jsx)(n.td,{children:"cohere.command-text-v14"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Cohere"}),(0,r.jsx)(n.td,{children:"Command Light"}),(0,r.jsx)(n.td,{children:"15.x"}),(0,r.jsx)(n.td,{children:"cohere.command-light-text-v14"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Cohere"}),(0,r.jsx)(n.td,{children:"Embed English"}),(0,r.jsx)(n.td,{children:"3.x"}),(0,r.jsx)(n.td,{children:"cohere.embed-english-v3"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Cohere"}),(0,r.jsx)(n.td,{children:"Embed Multilingual"}),(0,r.jsx)(n.td,{children:"3.x"}),(0,r.jsx)(n.td,{children:"cohere.embed-multilingual-v3"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Meta"}),(0,r.jsx)(n.td,{children:"Llama 2 Chat 13B"}),(0,r.jsx)(n.td,{children:"1.x"}),(0,r.jsx)(n.td,{children:"meta.llama2-13b-chat-v1"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Meta"}),(0,r.jsx)(n.td,{children:"Llama 2 Chat 70B"}),(0,r.jsx)(n.td,{children:"1.x"}),(0,r.jsx)(n.td,{children:"meta.llama2-70b-chat-v1"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Mistral AI"}),(0,r.jsx)(n.td,{children:"Mistral 7B Instruct"}),(0,r.jsx)(n.td,{children:"0.x"}),(0,r.jsx)(n.td,{children:"mistral.mistral-7b-instruct-v0:2"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Mistral AI"}),(0,r.jsx)(n.td,{children:"Mixtral 8X7B Instruct"}),(0,r.jsx)(n.td,{children:"0.x"}),(0,r.jsx)(n.td,{children:"mistral.mixtral-8x7b-instruct-v0:1"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Mistral AI"}),(0,r.jsx)(n.td,{children:"Mistral Large"}),(0,r.jsx)(n.td,{children:"1.x"}),(0,r.jsx)(n.td,{children:"mistral.mistral-large-2402-v1:0"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Stability AI"}),(0,r.jsx)(n.td,{children:"Stable Diffusion XL"}),(0,r.jsx)(n.td,{children:"0.x"}),(0,r.jsx)(n.td,{children:"stability.stable-diffusion-xl-v0"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Stability AI"}),(0,r.jsx)(n.td,{children:"Stable Diffusion XL"}),(0,r.jsx)(n.td,{children:"1.x"}),(0,r.jsx)(n.td,{children:"stability.stable-diffusion-xl-v1"})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"basic-patterns",children:"Basic patterns"}),"\n",(0,r.jsx)(n.h3,{id:"b1-text-generation",children:"B1 Text Generation"}),"\n",(0,r.jsx)(n.p,{children:"Putting together a streamlit app that does text-to-text generation for us."}),"\n",(0,r.jsx)(n.p,{children:"Creating 2 x files"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"text_lib.py"})," # the backend functions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"text_app.py"})," # the frontend UI"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Backend Functions"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="text_lib.py" ShowLineNumbers',children:'from langchain_community.llms import Bedrock\n\ndef get_text_response(input_content): #text-to-text client function\n\n    llm = Bedrock( #create a Bedrock llm client\n        model_id="cohere.command-text-v14", #set the foundation model\n        model_kwargs={\n            "max_tokens": 512,\n            "temperature": 0,\n            "p": 0.01,\n            "k": 0,\n            "stop_sequences": [],\n            "return_likelihoods": "NONE"\n        }\n    )\n    return llm.invoke(input_content) #return a response to the prompt\n\n'})}),"\n",(0,r.jsx)(n.p,{children:"The streamlit UI"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="text_app.py" ShowLineNumbers',children:'import streamlit as st\nimport text_lib as glib\n\n# Titles\nst.set_page_config(page_title="Text to Text")\nst.title("Text to Text") \n\n# Inputs\ninput_text = st.text_area("Input text", label_visibility="collapsed")\ngo_button = st.button("Go", type="primary")\n\n# Outputs\nif go_button:\n  #show a spinner while the code in this with block runs\n  with st.spinner("Working..."):\n    #call the model through the supporting library\n    response_content = glib.get_text_response(input_content=input_text)\n    #display the response content\n    st.write(response_content)\n'})}),"\n",(0,r.jsxs)(n.p,{children:["Run it: ",(0,r.jsx)(n.code,{children:"streamlit run text_app.py --server.port 8080"})]}),"\n",(0,r.jsx)(n.p,{children:"Success"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"streamlit ui",src:t(98058).A+"",width:"750",height:"709"})}),"\n",(0,r.jsx)(n.h3,{id:"b2-image-generation",children:"B2 Image Generation"}),"\n",(0,r.jsxs)(n.p,{children:["Same as text generation, we have a ",(0,r.jsx)(n.code,{children:"_lib.py"})," file (backend) and an ",(0,r.jsx)(n.code,{children:"_app.py"})," file (frontend)"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="image_lib.py" ShowLineNumbers',children:'import boto3 #import aws sdk and supporting libraries\nimport json\nimport base64\nfrom io import BytesIO\n\n# init client, bedrock id\nsession = boto3.Session()\nbedrock = session.client(service_name=\'bedrock-runtime\') #creates a Bedrock client\nbedrock_model_id = "stability.stable-diffusion-xl-v1" #use the Stable Diffusion model\n\n# convert reponse to streamlit can display\ndef get_response_image_from_payload(response): #returns the image bytes from the model response payload\n\n    payload = json.loads(response.get(\'body\').read()) #load the response body into a json object\n    images = payload.get(\'artifacts\') #extract the image artifacts\n    image_data = base64.b64decode(images[0].get(\'base64\')) #decode image\n\n    return BytesIO(image_data) #return a BytesIO object for client app consumption\n\n# call bedrock from UI\ndef get_image_response(prompt_content): #text-to-text client function\n    \n    request_body = json.dumps({"text_prompts": \n                               [ {"text": prompt_content } ], #prompts to use\n                               "cfg_scale": 9, #how closely the model tries to match the prompt\n                               "steps": 50, }) #number of diffusion steps to perform\n    \n    response = bedrock.invoke_model(body=request_body, modelId=bedrock_model_id) #call the Bedrock endpoint\n    \n    output = get_response_image_from_payload(response) #convert the response payload to a BytesIO object for the client to consume\n    \n    return output\n'})}),"\n",(0,r.jsx)(n.p,{children:"Frontend"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="image_app.py" ShowLineNumbers',children:'import streamlit as st #all streamlit commands will be available through the "st" alias\nimport image_lib as glib #reference to local lib script\n\nst.set_page_config(layout="wide", page_title="Image Generation") #set the page width wider to accommodate columns\nst.title("Image Generation") #page title\ncol1, col2 = st.columns(2) #create 2 columns\n\nwith col1: #everything in this with block will be placed in column 1\n    st.subheader("Image generation prompt") #subhead for this column    \n    prompt_text = st.text_area("Prompt text", height=200, label_visibility="collapsed") #display a multiline text box with no label\n    process_button = st.button("Run", type="primary") #display a primary button\n\nwith col2: #everything in this with block will be placed in column 2\n    st.subheader("Result") #subhead for this column    \n    if process_button: #code in this if block will be run when the button is clicked\n        with st.spinner("Drawing..."): #show a spinner while the code in this with block runs\n            generated_image = glib.get_image_response(prompt_content=prompt_text) #call the model through the supporting library\n        st.image(generated_image) #display the generated image\n'})}),"\n",(0,r.jsxs)(n.p,{children:["Run it: ",(0,r.jsx)(n.code,{children:"streamlit run text_app.py --server.port 8080"})]}),"\n",(0,r.jsx)(n.p,{children:"Success"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"streamlit ui",src:t(55489).A+"",width:"964",height:"608"})}),"\n",(0,r.jsx)(n.h3,{id:"b3-rag",children:"B3 RAG"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="rag_lib.py" showLineNumbers',children:'from langchain_community.embeddings import BedrockEmbeddings\nfrom langchain.indexes import VectorstoreIndexCreator\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_community.llms import Bedrock\n\ndef get_llm():\n    \n    model_kwargs = { #AI21\n        "maxTokens": 1024, \n        "temperature": 0, \n        "topP": 0.5, \n        "stopSequences": [], \n        "countPenalty": {"scale": 0 }, \n        "presencePenalty": {"scale": 0 }, \n        "frequencyPenalty": {"scale": 0 } \n    }\n    \n    llm = Bedrock(\n        model_id="ai21.j2-ultra-v1", #set the foundation model\n        model_kwargs=model_kwargs) #configure the properties for Claude\n    \n    return llm\n\ndef get_index(): #creates and returns an in-memory vector store to be used in the application\n    \n    embeddings = BedrockEmbeddings() #create a Titan Embeddings client\n    pdf_path = "2022-Shareholder-Letter.pdf" #assumes local PDF file with this name\n    loader = PyPDFLoader(file_path=pdf_path) #load the pdf file\n  \n    text_splitter = RecursiveCharacterTextSplitter( #create a text splitter\n        separators=["\\n\\n", "\\n", ".", " "], #split chunks at (1) paragraph, (2) line, (3) sentence, or (4) word, in that order\n        chunk_size=1000, #divide into 1000-character chunks using the separators above\n        chunk_overlap=100 #number of characters that can overlap with previous chunk\n    )\n    \n    index_creator = VectorstoreIndexCreator( #create a vector store factory\n        vectorstore_cls=FAISS, #use an in-memory vector store for demo purposes\n        embedding=embeddings, #use Titan embeddings\n        text_splitter=text_splitter, #use the recursive text splitter\n    )\n    \n    index_from_loader = index_creator.from_loaders([loader]) #create an vector store index from the loaded PDF\n    \n    return index_from_loader #return the index to be cached by the client app\n\ndef get_rag_response(index, question): #rag client function\n    \n    llm = get_llm()  \n    response_text = index.query(question=question, llm=llm) #search against the in-memory index, stuff results into a prompt and send to the llm\n    \n    return response_text\n'})}),"\n",(0,r.jsxs)(n.p,{children:["streamlit app ",(0,r.jsx)(n.code,{children:"rag_app.py"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="rag_app.py" showLineNumbers',children:'\nimport streamlit as st #all streamlit commands will be available through the "st" alias\nimport rag_lib as glib #reference to local lib script\n\n# Titles\nst.set_page_config(page_title="Retrieval-Augmented Generation") #HTML title\nst.title("Retrieval-Augmented Generation") #page title\n\n# Vector Index\nif \'vector_index\' not in st.session_state: #see if the vector index hasn\'t been created yet\n    with st.spinner("Indexing document..."): #show a spinner while the code in this with block runs\n        st.session_state.vector_index = glib.get_index() #retrieve the index through the supporting library and store in the app\'s session cache\n\n# Inputs\ninput_text = st.text_area("Input text", label_visibility="collapsed") #display a multiline text box with no label\ngo_button = st.button("Go", type="primary") #display a primary button\n\n# Outputs\nif go_button: #code in this if block will be run when the button is clicked\n    \n    with st.spinner("Working..."): #show a spinner while the code in this with block runs\n        response_content = glib.get_rag_response(index=st.session_state.vector_index, question=input_text) #call the model through the supporting library\n        \n        st.write(response_content) #display the response content\n'})}),"\n",(0,r.jsx)(n.p,{children:"add requirements: faiss-cpu"}),"\n",(0,r.jsxs)(n.p,{children:["Run it: ",(0,r.jsx)(n.code,{children:"streamlit run rag_app.py --server.port 8080"})]}),"\n",(0,r.jsx)(n.p,{children:"Success"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"RAG",src:t(34360).A+"",width:"1506",height:"688"})}),"\n",(0,r.jsx)(n.h3,{id:"b4-chatbot",children:"B4 Chatbot"}),"\n",(0,r.jsx)(n.p,{children:"Create backend functions"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="chatbot_lib.py" showLineNumbers',children:'from langchain.memory import ConversationSummaryBufferMemory\nfrom langchain_community.chat_models import BedrockChat\nfrom langchain.chains import ConversationChain\n\n# setup LLM\ndef get_llm():\n        \n    model_kwargs = { #anthropic\n        "max_tokens": 512,\n        "temperature": 0, \n        "top_k": 250, \n        "top_p": 1, \n        "stop_sequences": ["\\n\\nHuman:"] \n    }\n    \n    llm = BedrockChat(\n        model_id="anthropic.claude-3-sonnet-20240229-v1:0", #set the foundation model\n        model_kwargs=model_kwargs) #configure the properties for Claude\n    \n    return llm\n\n# init a langchain memory object\ndef get_memory(): #create memory for this chat session\n    \n    #ConversationSummaryBufferMemory requires an LLM for summarizing older messages\n    #this allows us to maintain the "big picture" of a long-running conversation\n    llm = get_llm()\n    \n    memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=1024) #Maintains a summary of previous messages\n    \n    return memory\n\n# call bedrock\ndef get_chat_response(input_text, memory): #chat client function\n    \n    llm = get_llm()\n    \n    conversation_with_summary = ConversationChain( #create a chat client\n        llm = llm, #using the Bedrock LLM\n        memory = memory, #with the summarization memory\n        verbose = True #print out some of the internal states of the chain while running\n    )\n    \n    chat_response = conversation_with_summary.invoke(input_text) #pass the user message and summary to the model\n    \n    return chat_response[\'response\']\n'})}),"\n",(0,r.jsx)(n.p,{children:"Setup frontend UI"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="chatbot_app.py" showLineNumbers',children:'\nimport streamlit as st #all streamlit commands will be available through the "st" alias\nimport chatbot_lib as glib #reference to local lib script\n\n# titles\n\nst.set_page_config(page_title="Chatbot") #HTML title\nst.title("Chatbot") #page title\n\n# add langchain memory to session cache\n\nif \'memory\' not in st.session_state: #see if the memory hasn\'t been created yet\n    st.session_state.memory = glib.get_memory() #initialize the memory\n\n# add ui chat history to session cache\n\nif \'chat_history\' not in st.session_state: #see if the chat history hasn\'t been created yet\n    st.session_state.chat_history = [] #initialize the chat history\n\n# render previous chat using a loop\n\nif \'chat_history\' not in st.session_state: #see if the chat history hasn\'t been created yet\n    st.session_state.chat_history = [] #initialize the chat history\n\n# Inputs\n\n\ninput_text = st.chat_input("Chat with your bot here") #display a chat input box\n\nif input_text: #run the code in this if block after the user submits a chat message\n    \n    with st.chat_message("user"): #display a user chat message\n        st.markdown(input_text) #renders the user\'s latest message\n    \n    st.session_state.chat_history.append({"role":"user", "text":input_text}) #append the user\'s latest message to the chat history\n    \n    chat_response = glib.get_chat_response(input_text=input_text, memory=st.session_state.memory) #call the model through the supporting library\n    \n    with st.chat_message("assistant"): #display a bot chat message\n        st.markdown(chat_response) #display bot\'s latest response\n    \n    st.session_state.chat_history.append({"role":"assistant", "text":chat_response}) #append the bot\'s latest message to the chat history\n'})}),"\n",(0,r.jsx)(n.p,{children:"add requirements: anthropic"}),"\n",(0,r.jsxs)(n.p,{children:["Run it: ",(0,r.jsx)(n.code,{children:"streamlit run chatbot_app.py --server.port 8080"})]}),"\n",(0,r.jsx)(n.p,{children:"Success"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"chatbot",src:t(56503).A+"",width:"1518",height:"1392"})}),"\n",(0,r.jsx)(n.h2,{id:"text-patterns",children:"Text Patterns"}),"\n",(0,r.jsx)(n.h3,{id:"t1-chatbot-rag",children:"T1 Chatbot RAG"}),"\n",(0,r.jsx)(n.p,{children:"Backend functions"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="" showLineNumbers',children:'from langchain.memory import ConversationBufferWindowMemory\nfrom langchain_community.chat_models import BedrockChat\nfrom langchain.chains import ConversationalRetrievalChain\n\nfrom langchain_community.embeddings import BedrockEmbeddings\nfrom langchain.indexes import VectorstoreIndexCreator\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import PyPDFLoader\n\n# setup llm\ndef get_llm():\n        \n    model_kwargs = { #anthropic\n        "max_tokens": 512,\n        "temperature": 0, \n        "top_k": 250, \n        "top_p": 1, \n        "stop_sequences": ["\\n\\nHuman:"] \n    }\n    \n    llm = BedrockChat(\n        model_id="anthropic.claude-3-sonnet-20240229-v1:0", #set the foundation model\n        model_kwargs=model_kwargs) #configure the properties for Claude\n    \n    return llm\n\n# in-memory vector store\n\ndef get_index(): #creates and returns an in-memory vector store to be used in the application\n    \n    embeddings = BedrockEmbeddings() #create a Titan Embeddings client\n    \n    pdf_path = "2022-Shareholder-Letter.pdf" #assumes local PDF file with this name\n\n    loader = PyPDFLoader(file_path=pdf_path) #load the pdf file\n    \n    text_splitter = RecursiveCharacterTextSplitter( #create a text splitter\n        separators=["\\n\\n", "\\n", ".", " "], #split chunks at (1) paragraph, (2) line, (3) sentence, or (4) word, in that order\n        chunk_size=1000, #divide into 1000-character chunks using the separators above\n        chunk_overlap=100 #number of characters that can overlap with previous chunk\n    )\n    \n    index_creator = VectorstoreIndexCreator( #create a vector store factory\n        vectorstore_cls=FAISS, #use an in-memory vector store for demo purposes\n        embedding=embeddings, #use Titan embeddings\n        text_splitter=text_splitter, #use the recursive text splitter\n    )\n    \n    index_from_loader = index_creator.from_loaders([loader]) #create an vector store index from the loaded PDF\n    \n    return index_from_loader #return the index to be cached by the client app\n\n# init langchain memory object\n\ndef get_memory(): #create memory for this chat session\n    \n    memory = ConversationBufferWindowMemory(memory_key="chat_history", return_messages=True) #Maintains a history of previous messages\n    \n    return memory\n\n# call bedrock\ndef get_rag_chat_response(input_text, memory, index): #chat client function\n    \n    llm = get_llm()\n    \n    conversation_with_retrieval = ConversationalRetrievalChain.from_llm(llm, index.vectorstore.as_retriever(), memory=memory, verbose=True)\n    \n    chat_response = conversation_with_retrieval.invoke({"question": input_text}) #pass the user message and summary to the model\n    \n    return chat_response[\'answer\']\n'})}),"\n",(0,r.jsx)(n.p,{children:"Frontend UI"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="rag_chatbot_app.py" showLineNumbers',children:'import streamlit as st #all streamlit commands will be available through the "st" alias\nimport rag_chatbot_lib as glib #reference to local lib script\n\n# titles\nst.set_page_config(page_title="RAG Chatbot") #HTML title\nst.title("RAG Chatbot") #page title\n\n# add langchain memory to session cache\nif \'memory\' not in st.session_state: #see if the memory hasn\'t been created yet\n    st.session_state.memory = glib.get_memory() #initialize the memory\n\n# add UI history to session cache\nif \'chat_history\' not in st.session_state: #see if the chat history hasn\'t been created yet\n    st.session_state.chat_history = [] #initialize the chat history\n\n# add vector index to session cache\nif \'vector_index\' not in st.session_state: #see if the vector index hasn\'t been created yet\n    with st.spinner("Indexing document..."): #show a spinner while the code in this with block runs\n        st.session_state.vector_index = glib.get_index() #retrieve the index through the supporting library and store in the app\'s session cache\n\n# Output - render chat history\n#Re-render the chat history (Streamlit re-runs this script, so need this to preserve previous chat messages)\nfor message in st.session_state.chat_history: #loop through the chat history\n    with st.chat_message(message["role"]): #renders a chat line for the given role, containing everything in the with block\n        st.markdown(message["text"]) #display the chat content\n\n# Inputs\ninput_text = st.chat_input("Chat with your bot here") #display a chat input box\n\nif input_text: #run the code in this if block after the user submits a chat message\n    \n    with st.chat_message("user"): #display a user chat message\n        st.markdown(input_text) #renders the user\'s latest message\n    \n    st.session_state.chat_history.append({"role":"user", "text":input_text}) #append the user\'s latest message to the chat history\n    \n    chat_response = glib.get_rag_chat_response(input_text=input_text, memory=st.session_state.memory, index=st.session_state.vector_index,) #call the model through the supporting library\n    \n    with st.chat_message("assistant"): #display a bot chat message\n        st.markdown(chat_response) #display bot\'s latest response\n    \n    st.session_state.chat_history.append({"role":"assistant", "text":chat_response}) #append the bot\'s latest message to the chat history\n\n\n'})}),"\n",(0,r.jsx)(n.p,{children:"add requirements: anthropic"}),"\n",(0,r.jsxs)(n.p,{children:["Run it: ",(0,r.jsx)(n.code,{children:"streamlit run chatbot_app.py --server.port 8080"})]}),"\n",(0,r.jsx)(n.p,{children:"Success"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"chatbot",src:t(76e3).A+"",width:"1604",height:"1408"})}),"\n",(0,r.jsx)(n.h3,{id:"t2-doc-summary",children:"T2 Doc Summary"}),"\n",(0,r.jsx)(n.p,{children:"Backend functions"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="summarization_lib.py" showLineNumbers',children:'from langchain.prompts import PromptTemplate\nfrom langchain_community.llms import Bedrock\nfrom langchain.chains.summarize import load_summarize_chain\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import PyPDFLoader\n\n# setup llm\ndef get_llm():\n    \n    model_kwargs = { #AI21\n        "maxTokens": 8000, \n        "temperature": 0, \n        "topP": 0.5, \n        "stopSequences": [], \n        "countPenalty": {"scale": 0 }, \n        "presencePenalty": {"scale": 0 }, \n        "frequencyPenalty": {"scale": 0 } \n    }\n    \n    llm = Bedrock(\n        model_id="ai21.j2-ultra-v1", #set the foundation model\n        model_kwargs=model_kwargs) #configure the properties for Claude\n    \n    return llm\n\n# create doc chunks of PDF\npdf_path = "2022-Shareholder-Letter.pdf"\n\ndef get_docs():\n    \n    loader = PyPDFLoader(file_path=pdf_path)\n    documents = loader.load()\n    text_splitter = RecursiveCharacterTextSplitter(\n        separators=["\\n\\n", "\\n", ".", " "], chunk_size=4000, chunk_overlap=100 \n    )\n    docs = text_splitter.split_documents(documents=documents)\n    \n    return docs\n\n# call bedrock\ndef get_summary(return_intermediate_steps=False):\n    \n    map_prompt_template = "{text}\\n\\nWrite a few sentences summarizing the above:"\n    map_prompt = PromptTemplate(template=map_prompt_template, input_variables=["text"])\n    \n    combine_prompt_template = "{text}\\n\\nWrite a detailed analysis of the above:"\n    combine_prompt = PromptTemplate(template=combine_prompt_template, input_variables=["text"])\n    \n    llm = get_llm()\n    docs = get_docs()\n    \n    chain = load_summarize_chain(llm, chain_type="map_reduce", map_prompt=map_prompt, combine_prompt=combine_prompt, return_intermediate_steps=return_intermediate_steps)\n    \n    if return_intermediate_steps:\n        return chain.invoke({"input_documents": docs}, return_only_outputs=True)\n    else:\n        return chain.invoke(docs, return_only_outputs=True)\n\n'})}),"\n",(0,r.jsx)(n.p,{children:"Frontend UI"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="summarization_app" showLineNumbers',children:'import streamlit as st\nimport summarization_lib as glib\n\n# titles\nst.set_page_config(page_title="Document Summarization")\nst.title("Document Summarization")\n\n# summarisation elements\nreturn_intermediate_steps = st.checkbox("Return intermediate steps", value=True)\nsummarize_button = st.button("Summarize", type="primary")\n\n\nif summarize_button:\n    st.subheader("Combined summary")\n    with st.spinner("Running..."):\n        response_content = glib.get_summary(return_intermediate_steps=return_intermediate_steps)\n\n\n    if return_intermediate_steps:\n        st.write(response_content["output_text"])\n        st.subheader("Section summaries")\n\n        for step in response_content["intermediate_steps"]:\n            st.write(step)\n            st.markdown("---")\n    else:\n        st.write(response_content["output_text"])\n'})}),"\n",(0,r.jsx)(n.p,{children:"add requirements: transformers"}),"\n",(0,r.jsxs)(n.p,{children:["Run it: ",(0,r.jsx)(n.code,{children:"streamlit run summarization_app.py --server.port 8080"})]}),"\n",(0,r.jsx)(n.p,{children:"Success"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"docsumm",src:t(14683).A+"",width:"1552",height:"1484"})}),"\n",(0,r.jsx)(n.h3,{id:"t3-response-streaming",children:"T3 Response Streaming"}),"\n",(0,r.jsx)(n.p,{children:"Backend functions"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="streaming_lib.py" showLineNumbers',children:'#imports\nfrom langchain.chains import ConversationChain\nfrom langchain_community.llms import Bedrock\n\n# setup llm\ndef get_llm(streaming_callback):\n    model_kwargs = {\n        "max_tokens": 4000,\n        "temperature": 0,\n        "p": 0.01,\n        "k": 0,\n        "stop_sequences": [],\n        "return_likelihoods": "NONE",\n        "stream": True\n    }\n    \n    llm = Bedrock(\n        model_id="cohere.command-text-v14",\n        model_kwargs=model_kwargs,\n        streaming=True,\n        callbacks=[streaming_callback],\n    )\n    \n    return llm\n\n# call bedrock, stream response\ndef get_streaming_response(prompt, streaming_callback):\n    conversation_with_summary = ConversationChain(\n        llm=get_llm(streaming_callback)\n    )\n    return conversation_with_summary.predict(input=prompt)\n'})}),"\n",(0,r.jsx)(n.p,{children:"Frontend UI"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="streaming_app.py" showLineNumbers',children:'import streaming_lib as glib  # reference to local lib script\nimport streamlit as st\nfrom langchain_community.callbacks.streamlit import StreamlitCallbackHandler # <<<<<\n\n# titles\nst.set_page_config(page_title="Response Streaming")  # HTML title\nst.title("Response Streaming")  # page title\n\n# Inputs\ninput_text = st.text_area("Input text", label_visibility="collapsed")\ngo_button = st.button("Go", type="primary")  # display a primary button\n\n# Outputs\nif go_button:  # code in this if block will be run when the button is clicked\n    #use an empty container for streaming output\n    st_callback = StreamlitCallbackHandler(st.container())\n    streaming_response = glib.get_streaming_response(prompt=input_text, streaming_callback=st_callback)\n'})}),"\n",(0,r.jsx)(n.p,{children:"add requirements: anthropic"}),"\n",(0,r.jsxs)(n.p,{children:["Run it: ",(0,r.jsx)(n.code,{children:"streamlit run streaming_app.py --server.port 8080"})]}),"\n",(0,r.jsx)(n.p,{children:"Success"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"response",src:t(65682).A+"",width:"1528",height:"954"})}),"\n",(0,r.jsx)(n.h3,{id:"t4-embeddings-search",children:"T4 Embeddings Search"}),"\n",(0,r.jsx)(n.p,{children:'This is similar to RAG setup, with one important distinction- the user query is a "search" of the vector database, and not generating a new result.'}),"\n",(0,r.jsx)(n.p,{children:"Note we're using in-memory FAISS vectorstore, in real world we'd use something more persistent."}),"\n",(0,r.jsx)(n.p,{children:"Backend functions"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="embeddings_search_lib" showLineNumbers',children:'#imports\nfrom langchain_community.embeddings import BedrockEmbeddings\nfrom langchain.indexes import VectorstoreIndexCreator\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_text_splitters import CharacterTextSplitter\nfrom langchain_community.document_loaders.csv_loader import CSVLoader\n\n# create in-memory store\ndef get_index(): #creates and returns an in-memory vector store to be used in the application\n    \n    embeddings = BedrockEmbeddings() #create a Titan Embeddings client\n    \n    loader = CSVLoader(file_path="sagemaker_answers.csv")\n\n    index_creator = VectorstoreIndexCreator(\n        vectorstore_cls=FAISS,\n        embedding=embeddings,\n        text_splitter=CharacterTextSplitter(chunk_size=300, chunk_overlap=0),\n    )\n\n    index_from_loader = index_creator.from_loaders([loader])\n    \n    return index_from_loader\n\n# call bedrock\ndef get_similarity_search_results(index, question):\n    results = index.vectorstore.similarity_search_with_score(question)\n    \n    flattened_results = [{"content":res[0].page_content, "score":res[1]} for res in results] #flatten results for easier display and handling\n    \n    return flattened_results\n\n# get embeddings\ndef get_embedding(text):\n    embeddings = BedrockEmbeddings() #create a Titan Embeddings client\n    \n    return embeddings.embed_query(text)\n'})}),"\n",(0,r.jsx)(n.p,{children:"Frontend UI"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="embeddings_search_app.py" showLineNumbers',children:'import streamlit as st #all streamlit commands will be available through the "st" alias\nimport embeddings_search_lib as glib #reference to local lib script\n\n# titles\nst.set_page_config(page_title="Embeddings Search", layout="wide") #HTML title\nst.title("Embeddings Search") #page title\n\n# add vector index to session cache\nif \'vector_index\' not in st.session_state: #see if the vector index hasn\'t been created yet\n    with st.spinner("Indexing document..."): #show a spinner while the code in this with block runs\n        st.session_state.vector_index = glib.get_index() #retrieve the index through the supporting library and store in the app\'s session cache\n\n# inputs\ninput_text = st.text_input("Ask a question about Amazon SageMaker:") #display a multiline text box with no label\ngo_button = st.button("Go", type="primary") #display a primary button\n\n# outputs\nif go_button: #code in this if block will be run when the button is clicked\n    \n    with st.spinner("Working..."): #show a spinner while the code in this with block runs\n        response_content = glib.get_similarity_search_results(index=st.session_state.vector_index, question=input_text)\n        \n        st.table(response_content) #using table so text will wrap\n        \n        \n        raw_embedding = glib.get_embedding(input_text)\n        \n        with st.expander("View question embedding"):\n            st.json(raw_embedding)\n'})}),"\n",(0,r.jsx)(n.p,{children:"add requirements: anthropic"}),"\n",(0,r.jsxs)(n.p,{children:["Run it: ",(0,r.jsx)(n.code,{children:"streamlit run embeddings_search_app.py --server.port 8080"})]}),"\n",(0,r.jsx)(n.p,{children:"Success"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"embeddings1",src:t(99184).A+"",width:"2666",height:"1398"})}),"\n",(0,r.jsx)(n.p,{children:"check out the embeddings values"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"embeddings2",src:t(64587).A+"",width:"2624",height:"1414"})}),"\n",(0,r.jsx)(n.h3,{id:"t5-personalised-recommendations",children:"T5 Personalised Recommendations"}),"\n",(0,r.jsx)(n.p,{children:'in a nutshell, user query -> RAG match query -> results go to LLM for "personalised summary".'}),"\n",(0,r.jsx)(n.p,{children:"Backend functions"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="recommendations_lib.py" showLineNumbers',children:'from langchain_community.llms import Bedrock\nfrom langchain_community.embeddings import BedrockEmbeddings\nfrom langchain.indexes import VectorstoreIndexCreator\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import JSONLoader\n\n# setup llm\ndef get_llm():\n    \n    model_kwargs = { #AI21\n        "maxTokens": 1024, \n        "temperature": 0, \n        "topP": 0.5, \n        "stopSequences": [], \n        "countPenalty": {"scale": 0 }, \n        "presencePenalty": {"scale": 0 }, \n        "frequencyPenalty": {"scale": 0 } \n    }\n    \n    llm = Bedrock(\n        model_id="ai21.j2-ultra-v1", #set the foundation model\n        model_kwargs=model_kwargs) #configure the properties for Claude\n    \n    return llm\n\n#function to identify the metadata to capture in the vectorstore and return along with the matched content\ndef item_metadata_func(record: dict, metadata: dict) -> dict: \n\n    metadata["name"] = record.get("name")\n    metadata["url"] = record.get("url")\n\n    return metadata\n\n# in memory vectory store\ndef get_index(): #creates and returns an in-memory vector store to be used in the application\n    \n    embeddings = BedrockEmbeddings() #create a Titan Embeddings client\n    \n    loader = JSONLoader(\n        file_path="services.json",\n        jq_schema=\'.[]\',\n        content_key=\'description\',\n        metadata_func=item_metadata_func)\n\n    text_splitter = RecursiveCharacterTextSplitter( #create a text splitter\n        separators=["\\n\\n", "\\n", ".", " "], #split chunks at (1) paragraph, (2) line, (3) sentence, or (4) word, in that order\n        chunk_size=8000, #based on this content, we just want the whole item so no chunking - this could lead to an error if the content is too long\n        chunk_overlap=0 #number of characters that can overlap with previous chunk\n    )\n    \n    index_creator = VectorstoreIndexCreator( #create a vector store factory\n        vectorstore_cls=FAISS, #use an in-memory vector store for demo purposes\n        embedding=embeddings, #use Titan embeddings\n        text_splitter=text_splitter, #use the recursive text splitter\n    )\n    \n    index_from_loader = index_creator.from_loaders([loader]) #create an vector store index from the loaded PDF\n    \n    return index_from_loader #return the index to be cached by the client app\n\n# call bedrock\ndef get_similarity_search_results(index, question):\n    raw_results = index.vectorstore.similarity_search_with_score(question)\n    \n    llm = get_llm()\n    \n    results = []\n    \n    for res in raw_results:\n        content = res[0].page_content\n        prompt = f"{content}\\n\\nSummarize how the above service addresses the following needs : {question}"\n        \n        summary = llm.invoke(prompt)\n        \n        results.append({"name": res[0].metadata["name"], "url": res[0].metadata["url"], "summary": summary, "original": content})\n    \n    return results\n\n\n'})}),"\n",(0,r.jsx)(n.p,{children:"Frontend UI"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="recommendations_app.py" showLineNumbers',children:'import streamlit as st #all streamlit commands will be available through the "st" alias\nimport recommendations_lib as glib #reference to local lib script\n\n# titles\nst.set_page_config(page_title="Personalized Recommendations", layout="wide") #HTML title\nst.title("Personalized Recommendations") #page title\n\n# add vector index to session cache\nif \'vector_index\' not in st.session_state: #see if the vector index hasn\'t been created yet\n    with st.spinner("Indexing document..."): #show a spinner while the code in this with block runs\n        st.session_state.vector_index = glib.get_index() #retrieve the index through the supporting library and store in the app\'s session cache\n\n# add inputs\ninput_text = st.text_input("Name some key features you need from a cloud service:") #display a multiline text box with no label\ngo_button = st.button("Go", type="primary") #display a primary button\n\n# add outputs\nif go_button: #code in this if block will be run when the button is clicked\n    \n    with st.spinner("Working..."): #show a spinner while the code in this with block runs\n        response_content = glib.get_similarity_search_results(index=st.session_state.vector_index, question=input_text)\n        \n        for result in response_content:\n            st.markdown(f"### [{result[\'name\']}]({result[\'url\']})")\n            st.write(result[\'summary\'])\n            with st.expander("Original"):\n                st.write(result[\'original\'])\n'})}),"\n",(0,r.jsx)(n.p,{children:"add requirements: jq"}),"\n",(0,r.jsxs)(n.p,{children:["Run it: ",(0,r.jsx)(n.code,{children:"streamlit run recommendations_app.py --server.port 8080"})]}),"\n",(0,r.jsx)(n.p,{children:"Success"}),"\n",(0,r.jsx)(n.p,{children:"You can see the recommendation summary compared to the full service documentation in the 'Original' section."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"personal",src:t(4740).A+"",width:"2692",height:"1436"})}),"\n",(0,r.jsx)(n.h3,{id:"t6-extract-json",children:"T6 Extract JSON"}),"\n",(0,r.jsx)(n.p,{children:"Backend functions"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="json_lib.py" showLineNumbers',children:'import json\nfrom json import JSONDecodeError\nfrom langchain_community.llms import Bedrock\n\n# get llm\ndef get_llm():\n\n    llm = Bedrock( #create a Bedrock llm client\n        model_id="ai21.j2-ultra-v1", #use the AI21 Jurassic-2 Ultra model\n        model_kwargs = {"maxTokens": 1024, "temperature": 0.0 } #for data extraction, minimum temperature is best\n    )\n\n    return llm\n\n# convert to JSON\ndef validate_and_return_json(response_text):\n    try:\n        response_json = json.loads(response_text) #attempt to load text into JSON\n        return False, response_json, None #returns has_error, response_content, err \n    \n    except JSONDecodeError as err:\n        return True, response_text, err #returns has_error, response_content, err \n\n# call bedrock\ndef get_json_response(input_content): #text-to-text client function\n    \n    llm = get_llm()\n\n    response = llm.invoke(input_content) #the text response for the prompt\n    \n    return validate_and_return_json(response)\n\n'})}),"\n",(0,r.jsx)(n.p,{children:"Frontend UI"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="json_app.py" showLineNumbers',children:'import streamlit as st #all streamlit commands will be available through the "st" alias\nimport json_lib as glib #reference to local lib script\n\n# titles\nst.set_page_config(page_title="Text to JSON", layout="wide")  #set the page width wider to accommodate columns\nst.title("Text to JSON")  #page title\ncol1, col2 = st.columns(2)  #create 2 columns\n\n# inputs, col layout left\nwith col1: #everything in this with block will be placed in column 1\n    st.subheader("Prompt") #subhead for this column\n    \n    input_text = st.text_area("Input text", height=500, label_visibility="collapsed")\n\n    process_button = st.button("Run", type="primary") #display a primary button\n\n# output, col layout right\nwith col2: #everything in this with block will be placed in column 2\n    st.subheader("Result") #subhead for this column\n    \n    if process_button: #code in this if block will be run when the button is clicked\n        with st.spinner("Running..."): #show a spinner while the code in this with block runs\n            has_error, response_content, err = glib.get_json_response(input_content=input_text) #call the model through the supporting library\n\n        if not has_error:\n            st.json(response_content) #render JSON if there was no error\n        else:\n            st.error(err) #otherwise render the error\n            st.write(response_content) #and render the raw response from the model\n\n'})}),"\n",(0,r.jsx)(n.p,{children:"add requirements:"}),"\n",(0,r.jsxs)(n.p,{children:["Run it: ",(0,r.jsx)(n.code,{children:"streamlit run json_app.py --server.port 8080"})]}),"\n",(0,r.jsx)(n.p,{children:"Success"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"JSON",src:t(34655).A+"",width:"1994",height:"1450"})}),"\n",(0,r.jsx)(n.h3,{id:"t7-text-to-csv",children:"T7 Text to CSV"}),"\n",(0,r.jsx)(n.p,{children:"Backend functions"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="csv_lib.py" showLineNumbers',children:'import pandas as pd\nfrom io import StringIO\nfrom langchain_community.llms import Bedrock\n\n# setup llm\ndef get_llm():\n\n    llm = Bedrock( #create a Bedrock llm client\n        model_id="ai21.j2-ultra-v1", #use the AI21 Jurassic-2 Ultra model\n        model_kwargs = {"maxTokens": 1024, "temperature": 0.0 } #for data extraction, minimum temperature is best\n    )\n\n    return llm\n\n# convert result to pandas dataframe\ndef validate_and_return_csv(response_text):\n    #returns has_error, response_content, err \n    try:\n        csv_io = StringIO(response_text)\n        return False, pd.read_csv(csv_io), None #attempt to load response CSV into a dataframe\n    \n    except Exception as err:\n        return True, response_text, err\n\n# call bedrock\ndef get_csv_response(input_content): #text-to-text client function\n    \n    llm = get_llm()\n\n    response = llm.invoke(input_content) #the text response for the prompt\n    \n    return validate_and_return_csv(response)\n'})}),"\n",(0,r.jsx)(n.p,{children:"Frontend UI"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="csv_app.py" showLineNumbers',children:'import streamlit as st #all streamlit commands will be available through the "st" alias\nimport csv_lib as glib #reference to local lib script\n\n# titles\nst.set_page_config(page_title="Text to CSV", layout="wide")  #set the page width wider to accommodate columns\nst.title("Text to CSV")  #page title\ncol1, col2 = st.columns(2)  #create 2 columns\n\n# inputs\nwith col1: #everything in this with block will be placed in column 1\n    st.subheader("Prompt") #subhead for this column\n    \n    input_text = st.text_area("Input text", height=500, label_visibility="collapsed")\n\n    process_button = st.button("Run", type="primary") #display a primary button\n\n# outputs col layout, result table top, raw data bottom\n\nwith col2: #everything in this with block will be placed in column 2\n    st.subheader("Result") #subhead for this column\n    \n    if process_button: #code in this if block will be run when the button is clicked\n        with st.spinner("Running..."): #show a spinner while the code in this with block runs\n            has_error, response_content, err = glib.get_csv_response(input_content=input_text) #call the model through the supporting library\n        \n        if not has_error:\n            st.dataframe(response_content)\n            \n            csv_content = response_content.to_csv(index = False)\n            \n            st.markdown("#### Raw CSV")\n            st.text(csv_content)\n            \n        else:\n            st.error(err)\n            st.write(response_content)\n'})}),"\n",(0,r.jsx)(n.p,{children:"add requirements: anthropic"}),"\n",(0,r.jsxs)(n.p,{children:["Run it: ",(0,r.jsx)(n.code,{children:"streamlit run csv_app.py --server.port 8080"})]}),"\n",(0,r.jsx)(n.p,{children:"Success"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"CSV",src:t(56566).A+"",width:"2452",height:"1570"})}),"\n",(0,r.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,r.jsx)(n.h3,{id:"error-messages",children:"Error Messages"}),"\n",(0,r.jsxs)(n.p,{children:["When I tried ",(0,r.jsx)(n.code,{children:'python3 ./params.py "anthropic.claude-v2" "Write a haiku:"'})]}),"\n",(0,r.jsx)(n.p,{children:"I got this error:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'Traceback (most recent call last):\n  File "/home/rxhackk/.local/lib/python3.10/site-packages/langchain_community/llms/bedrock.py", line 444, in _prepare_input_and_invoke\n    response = self.client.invoke_model(**request_options)\n  File "/home/rxhackk/.local/lib/python3.10/site-packages/botocore/client.py", line 553, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File "/home/rxhackk/.local/lib/python3.10/site-packages/botocore/client.py", line 1009, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.errorfactory.ValidationException: An error occurred (ValidationException) when calling the InvokeModel operation: Malformed input request: #: extraneous key [max_tokens] is not permitted, please reformat your input and try again.\n'})}),"\n",(0,r.jsxs)(n.p,{children:["As at ",(0,r.jsx)(n.code,{children:"April 6th, 2024"})," the models parameters have been updated (per ",(0,r.jsx)(n.a,{href:"https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/providers?model=anthropic.claude-v2:1",children:"docs"}),") to the following:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'{\n  "modelId": "anthropic.claude-v2:1",\n  "contentType": "application/json",\n  "accept": "*/*",\n  "body": "{\\"prompt\\":\\"\\\\n\\\\nHuman: Hello world\\\\n\\\\nAssistant:\\",\\"max_tokens_to_sample\\":300,\\"temperature\\":0.5,\\"top_k\\":250,\\"top_p\\":1,\\"stop_sequences\\":[\\"\\\\n\\\\nHuman:\\"],\\"anthropic_version\\":\\"bedrock-2023-05-31\\"}"\n}\n'})}),"\n",(0,r.jsx)(n.p,{children:"I tested the other models, and their default params haven't changed:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",metastring:'title="cohere"',children:'/workshop/labs/params \u276f python3 ./params.py "cohere.command-text-v14" "Write a haiku:"                    \ue73c .env at \uf017 12:11:23\n Haiku is a form of Japanese poetry that consists of three lines. The first line has five syllables, the second line has seven syllables, and the third line has five syllables. Here is an example of a haiku:\n\nSpring rain opening\nthe silent flowers after\na cold, dry winter\n\nWould you like me to write another haiku for you? \n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",metastring:'title="meta"',children:'/workshop/labs/params \u276f python3 ./params.py "meta.llama2-13b-chat-v1" "Write a haiku:"          took \uf252 4s \ue73c .env at \uf017 12:16:26\nThe sun sets slowly\nGolden hues upon the sea\nPeaceful evening sky\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",metastring:'title="mistral"',children:'/workshop/labs/params \u276f python3 ./params.py "mistral.mistral-7b-instruct-v0:2" "Write a haiku:"           \ue73c .env at \uf017 12:17:09\n\nAutumn leaves fall slow\nWhispers of the wind\u2019s song\nNature\u2019s symphony\n\nHaiku is a form of traditional Japanese poetry. It consists of three lines with a 5-7-5 syllable count. The haiku should capture a moment in nature and convey a sense of seasonality and imagery. In this haiku, I have tried to capture the feeling of autumn leaves falling slowly and the sound of the wind as it rustles through them. The phrase "Nature\'s symphony" is used to emphasize the beauty and harmony of the natural world during this season.\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",metastring:'title="amazon"',children:'python3 ./params.py "amazon.titan-text-express-v1" "Write a haiku:"                                                                                         \ue73c .env at \uf017 12:17:31\nI am a\nI am a bookworm\nI read a lot\n'})})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},34360:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/AWSBedrockLangchainWK-LabB3-4d2c3d0d1c5c497715543abc416cf22e.png"},34655:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/AWSBedrockLangchainWK-LabT6-d152a716e8e701f700e3c7c72e83d8ac.png"},55489:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/AWSBedrockLangchainWK-LabB2-add972fc4e12f75faf5d92a57a3f44f6.png"},56503:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/AWSBedrockLangchainWK-LabB4-806be21fb265dfa2f738c01ac347ee5b.png"},56566:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/AWSBedrockLangchainWK-LabT7-7811ac00fecb61465f945ab0a962cb5e.png"},59893:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/AWSBedrockLangchainWK-Header-c810a0e4964e0936730fa7772519eac5.png"},64587:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/AWSBedrockLangchainWK-LabT42-b6fb60caf5ec1d950a9c3104551246b1.png"},65682:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/AWSBedrockLangchainWK-LabT3-ea9e2f93b6959bf9afd2418b21845a82.png"},76e3:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/AWSBedrockLangchainWK-LabT1-39a96dee18fcb569d91aa951d6a27585.png"},98058:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/AWSBedrockLangchainWK-LabB1-981e6a9a4f029505d2cbc1c5dac22ad2.png"},99184:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/AWSBedrockLangchainWK-LabT41-b7e5d4c3abd30db4a1c3d4af231bd10c.png"}}]);