"use strict";(self.webpackChunkronamosa_github_io=self.webpackChunkronamosa_github_io||[]).push([[8967],{4766:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/Mistral7B-JupyterLab-Space-Mistral7B-d4bb4b0a0380c8e4e395c11712e9a817.png"},13134:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/Mistral7B-JupyterLab-Space-Run-bc87976d0bcd694742378da5cf34bb3d.png"},14437:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/Mistral7B-JupyterLab-Space-Launcher-fe90689699bcbb094c472696b424f98d.png"},24183:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/Mistral7B-SageMaker-eb831f5061c59071d33624b1c78351a5.png"},26347:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/Mistral7B-JupyterLab-Space-Deps-88905789c7c5d25fb19cb802778673b5.png"},28453:(e,n,a)=>{a.d(n,{R:()=>t,x:()=>l});var s=a(96540);const i={},r=s.createContext(i);function t(e){const n=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:t(e.components),s.createElement(r.Provider,{value:n},e.children)}},31122:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/Mistral7B-JupyterLab-Space-Settings-9abcec1dd2853316d910febe08661e80.png"},41433:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/Mistral7B-JupyterLab-Space-Open-1dcaba116a7c69da82cf679b6c752a7a.png"},47619:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"engineer/AI/Mistral-7B-SageMaker","title":"Deploy Mistral-7B LLM with Ollama on AWS SageMaker","description":"Complete tutorial for deploying and running Mistral-7B large language model using Ollama on AWS SageMaker. Includes setup, configuration, and usage examples.","source":"@site/docs/engineer/AI/Mistral-7B-SageMaker.md","sourceDirName":"engineer/AI","slug":"/engineer/AI/Mistral-7B-SageMaker","permalink":"/docs/engineer/AI/Mistral-7B-SageMaker","draft":false,"unlisted":false,"editUrl":"https://github.com/ronamosa/ronamosa.github.io/edit/main/website/docs/engineer/AI/Mistral-7B-SageMaker.md","tags":[{"inline":true,"label":"ai","permalink":"/docs/tags/ai"},{"inline":true,"label":"llm","permalink":"/docs/tags/llm"},{"inline":true,"label":"aws","permalink":"/docs/tags/aws"},{"inline":true,"label":"sagemaker","permalink":"/docs/tags/sagemaker"},{"inline":true,"label":"mistral","permalink":"/docs/tags/mistral"},{"inline":true,"label":"ollama","permalink":"/docs/tags/ollama"}],"version":"current","lastUpdatedBy":"Ron Amosa","lastUpdatedAt":1758526302000,"sidebarPosition":1,"frontMatter":{"title":"Deploy Mistral-7B LLM with Ollama on AWS SageMaker","description":"Complete tutorial for deploying and running Mistral-7B large language model using Ollama on AWS SageMaker. Includes setup, configuration, and usage examples.","keywords":["mistral-7b","ollama","aws sagemaker","llm deployment","large language model","ai","machine learning","aws ai"],"tags":["ai","llm","aws","sagemaker","mistral","ollama"],"sidebar_position":1},"sidebar":"docsSidebar","previous":{"title":"AI & Machine Learning Projects Hub: LLM Deployment & GenAI","permalink":"/docs/engineer/AI"},"next":{"title":"PrivateGPT Local Deployment on Linux - Secure Document Chat Without Cloud","permalink":"/docs/engineer/AI/PrivateGPT"}}');var i=a(74848),r=a(28453);const t={title:"Deploy Mistral-7B LLM with Ollama on AWS SageMaker",description:"Complete tutorial for deploying and running Mistral-7B large language model using Ollama on AWS SageMaker. Includes setup, configuration, and usage examples.",keywords:["mistral-7b","ollama","aws sagemaker","llm deployment","large language model","ai","machine learning","aws ai"],tags:["ai","llm","aws","sagemaker","mistral","ollama"],sidebar_position:1},l=void 0,o={},d=[{value:"Related AI &amp; Machine Learning Projects",id:"related-ai--machine-learning-projects",level:2},{value:"Login to AWS",id:"login-to-aws",level:2},{value:"Install Dependencies",id:"install-dependencies",level:2},{value:"JupyterLab Notebook",id:"jupyterlab-notebook",level:2},{value:"Install transformers",id:"install-transformers",level:3},{value:"Install Ollama",id:"install-ollama",level:3},{value:"Run Ollama",id:"run-ollama",level:2},{value:"Download Mistral7B",id:"download-mistral7b",level:2},{value:"Resources",id:"resources",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsx)(n.p,{children:"Playing with OpenSource LLMs on some super-powered AWS instances."})}),"\n",(0,i.jsx)(n.h2,{id:"related-ai--machine-learning-projects",children:"Related AI & Machine Learning Projects"}),"\n",(0,i.jsxs)(n.p,{children:["\ud83e\udd16 ",(0,i.jsx)(n.strong,{children:"AI Infrastructure & Deployment"}),": Expand your AI/ML capabilities:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Local AI Setup"}),": ",(0,i.jsx)(n.a,{href:"/docs/engineer/AI/PrivateGPT",children:"PrivateGPT Local Deployment"})," - Privacy-focused AI on your own hardware"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AWS Private AI"}),": ",(0,i.jsx)(n.a,{href:"/docs/engineer/AI/PrivateGPTAWS",children:"PrivateGPT on AWS"})," - Secure cloud-based AI deployment"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Production AI"}),": ",(0,i.jsx)(n.a,{href:"/docs/engineer/AI/DeployLLMToSageMaker",children:"Deploy LLMs to SageMaker"})," - Scale AI models in production"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AI Services Integration"}),": ",(0,i.jsx)(n.a,{href:"/docs/engineer/AI/BedrockLangChainWorkshop1",children:"AWS Bedrock + LangChain"})," - Enterprise AI workflows"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["\ud83c\udfd7\ufe0f ",(0,i.jsx)(n.strong,{children:"Infrastructure Context"}),": Power your AI workloads with robust infrastructure:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AI Hub"}),": ",(0,i.jsx)(n.a,{href:"/docs/engineer/AI/ai-projects-hub",children:"AI & Machine Learning Projects Hub"})," - Complete AI project collection and roadmap"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Infrastructure Hosting"}),": ",(0,i.jsx)(n.a,{href:"/docs/engineer/LAB/home-lab-hub",children:"Home Lab Infrastructure Hub"})," - Self-hosted AI infrastructure"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cloud Integration"}),": ",(0,i.jsx)(n.a,{href:"/docs/engineer/AWS/",children:"AWS Services"})," - Cloud-native AI deployment strategies"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"login-to-aws",children:"Login to AWS"}),"\n",(0,i.jsx)(n.p,{children:"Login to your AWS account."}),"\n",(0,i.jsxs)(n.p,{children:["Go to SageMaker, Studio, click ",(0,i.jsx)(n.code,{children:"Open Studio"})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"AWS SageMaker console dashboard showing Studio access button for launching machine learning environment",src:a(24183).A+"",width:"1622",height:"417"})}),"\n",(0,i.jsx)(n.p,{children:"This is SageMaker Studio"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"SageMaker Studio main interface displaying JupyterLab spaces and development environment options",src:a(62576).A+"",width:"1908",height:"960"})}),"\n",(0,i.jsx)(n.p,{children:"View JupyterLab spaces"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"SageMaker JupyterLab spaces overview showing available computing environments for machine learning projects",src:a(62911).A+"",width:"1908",height:"960"})}),"\n",(0,i.jsx)(n.p,{children:'Create JupyterLab space - "OpenSourceLLM"'}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Create new JupyterLab space dialog for setting up OpenSource LLM development environment",src:a(83262).A+"",width:"1920",height:"1053"})}),"\n",(0,i.jsx)(n.p,{children:"In the workspace, I chose:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Instance=",(0,i.jsx)(n.code,{children:"ml.g4dn.xlarge"})]}),"\n",(0,i.jsxs)(n.li,{children:["Image=",(0,i.jsx)(n.code,{children:"SageMaker Distribution 1.2"})]}),"\n",(0,i.jsxs)(n.li,{children:["Storage(GB)=",(0,i.jsx)(n.code,{children:"100"}),", ",(0,i.jsx)(n.code,{children:"Run space"})]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Sagemaker",src:a(31122).A+"",width:"1920",height:"1053"})}),"\n",(0,i.jsxs)(n.p,{children:["Run your configured space ",(0,i.jsx)(n.code,{children:"Run space"})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Sagemaker",src:a(13134).A+"",width:"1920",height:"1053"})}),"\n",(0,i.jsxs)(n.p,{children:["Ready to ",(0,i.jsx)(n.code,{children:"Open JupyterLab"})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Sagemaker",src:a(41433).A+"",width:"1920",height:"1053"})}),"\n",(0,i.jsxs)(n.p,{children:["You're now in your OpenSourceLLM JupyterLab Workspace (note I changed light to dark theme under ",(0,i.jsx)(n.code,{children:"settings"}),"):"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Sagemaker",src:a(14437).A+"",width:"1920",height:"1053"})}),"\n",(0,i.jsx)(n.h2,{id:"install-dependencies",children:"Install Dependencies"}),"\n",(0,i.jsxs)(n.p,{children:["From ",(0,i.jsx)(n.code,{children:"Other"})," section, choose ",(0,i.jsx)(n.code,{children:"Terminal"})]}),"\n",(0,i.jsx)(n.p,{children:"Run the following command to install build tools:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"sudo apt-get update\nsudo apt-get install -y make build-essential libssl-dev zlib1g-dev libbz2-dev \\\n    libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev libncursesw5-dev \\\n    xz-utils tk-dev libffi-dev liblzma-dev git lshw\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Sagemaker",src:a(26347).A+"",width:"916",height:"903"})}),"\n",(0,i.jsxs)(n.admonition,{type:"caution",children:[(0,i.jsxs)(n.p,{children:["if you don't have ",(0,i.jsx)(n.code,{children:"lshw"})," installed ollama will give you this warning:"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'>>> Installing ollama to /usr/local/bin...\nWARNING: Unable to detect NVIDIA GPU. Install lspci or lshw to automatically detect and install NVIDIA CUDA drivers.\n>>> The Ollama API is now available at 0.0.0.0:11434.\n>>> Install complete. Run "ollama" from the command line.\n'})})]}),"\n",(0,i.jsx)(n.h2,{id:"jupyterlab-notebook",children:"JupyterLab Notebook"}),"\n",(0,i.jsxs)(n.p,{children:["From ",(0,i.jsx)(n.code,{children:"Notebook"})," options, choose ",(0,i.jsx)(n.code,{children:"Python 3 (ipykernel)"})]}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsxs)(n.p,{children:["I renamed the notebook, press F2, to ",(0,i.jsx)(n.code,{children:"Mistral7B"}),")"]})}),"\n",(0,i.jsx)(n.h3,{id:"install-transformers",children:"Install transformers"}),"\n",(0,i.jsxs)(n.p,{children:["Run: ",(0,i.jsx)(n.code,{children:"!pip install transformers"})," in first cell"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# output\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.31.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.10.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"install-ollama",children:"Install Ollama"}),"\n",(0,i.jsxs)(n.p,{children:["Type ",(0,i.jsx)(n.code,{children:"b"}),", shortcut to add another cell underneath."]}),"\n",(0,i.jsxs)(n.p,{children:["Run: ",(0,i.jsx)(n.code,{children:"!curl https://ollama.ai/install.sh | sh"})," to install ollama"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# output\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0>>> Downloading ollama...\n100  8354    0  8354    0     0  24587      0 --:--:-- --:--:-- --:--:-- 24570\n######################################################################## 100.0%##O#-#                                                                        \n>>> Installing ollama to /usr/local/bin...\n>>> NVIDIA GPU installed.\n>>> The Ollama API is now available at 0.0.0.0:11434.\n>>> Install complete. Run "ollama" from the command line.\n'})}),"\n",(0,i.jsx)(n.p,{children:"This will look like the following in your Notebook:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Sagemaker",src:a(47870).A+"",width:"1602",height:"913"})}),"\n",(0,i.jsx)(n.h2,{id:"run-ollama",children:"Run Ollama"}),"\n",(0,i.jsx)(n.p,{children:"Go back to your Terminal window."}),"\n",(0,i.jsxs)(n.p,{children:["Run ollama server: ",(0,i.jsx)(n.code,{children:"ollama serve"})]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# output\n\nsagemaker-user@default:~$ ollama serve\nCouldn't find '/home/sagemaker-user/.ollama/id_ed25519'. Generating new private key.\nYour new public key is: \n\nssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIIxhdvgcRFe7Ic1ZC9zPqW/5LPaBiDYH4SlXLhigZ9mW\n\n2024/01/17 09:43:58 images.go:808: total blobs: 0\n2024/01/17 09:43:58 images.go:815: total unused blobs removed: 0\n2024/01/17 09:43:58 routes.go:930: Listening on 127.0.0.1:11434 (version 0.1.20)\n2024/01/17 09:43:58 shim_ext_server.go:142: Dynamic LLM variants [rocm cuda]\n2024/01/17 09:43:58 gpu.go:88: Detecting GPU type\n2024/01/17 09:43:58 gpu.go:203: Searching for GPU management library libnvidia-ml.so\n2024/01/17 09:43:58 gpu.go:248: Discovered GPU libraries: [/usr/lib/x86_64-linux-gnu/libnvidia-ml.so.535.129.03]\n2024/01/17 09:44:00 gpu.go:94: Nvidia GPU detected\n2024/01/17 09:44:00 gpu.go:135: CUDA Compute Capability detected: 7.5\n"})}),"\n",(0,i.jsx)(n.p,{children:"Ollama server is running and ready for commands."}),"\n",(0,i.jsx)(n.h2,{id:"download-mistral7b",children:"Download Mistral7B"}),"\n",(0,i.jsxs)(n.p,{children:["Go back to Notebook and run: ",(0,i.jsx)(n.code,{children:"!ollama run mistral"})]}),"\n",(0,i.jsx)(n.p,{children:"Ollama will pull down the Mistral7B LLM, until you see:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"writing manifest \nremoving any unused layers \nsuccess\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Open a new Terminal, and check ollama with ",(0,i.jsx)(n.code,{children:"ollama list"})]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"sagemaker-user@default:~$ ollama list\nNAME            ID              SIZE    MODIFIED      \nmistral:latest  61e88e884507    4.1 GB  3 minutes ago\nsagemaker-user@default:~$ \n"})}),"\n",(0,i.jsxs)(n.p,{children:["Now run mistral with: ",(0,i.jsx)(n.code,{children:"ollama run mistral:latest"})]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# output\n\nsagemaker-user@default:~$ ollama run mistral:latest\n>>> hi, who is the greatest MMA fighter of all time?\n Determining the greatest MMA (Mixed Martial Arts) fighter of all time is a subjective question as it depends on various factors such as individual fighting styles, eras, weight classes, and personal preferences. \nSome popular names often mentioned in this discussion are:\n\n1. Anderson Silva: A former UFC Middleweight champion known for his devastating strikes and impeccable striking defense. He held the middleweight title for an record-breaking 2,457 days.\n2. Georges St-Pierre: A former UFC Welterweight and Middleweight champion with an impressive record of 26 wins and 2 losses. He is known for his versatility in various martial arts and excellent conditioning.\n3. Khabib Nurmagomedov: A current UFC Lightweight champion with a perfect 29-0 record. Known for his grappling skills, wrestling background, and exceptional ground control.\n4. Jon Jones: A former UFC Light Heavyweight and Heavyweight champion with a record of 26 wins and 1 loss. He is known for his striking abilities, agility, and grappling skills.\n5. Fedor Emelianenko: A legendary fighter from the Pride FC era with a professional MMA record of 43 wins (37 KOs, 6 ND, 0 L) and 5 losses. He is known for his devastating strikes, particularly in Sambo and Judo.\n\nThere are several other fighters who have made significant contributions to the sport as well. Ultimately, the "greatest" MMA fighter is a matter of personal opinion.\n\n>>> Send a message (/? for help)\n'})}),"\n",(0,i.jsx)(n.p,{children:"Success."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Sagemaker",src:a(4766).A+"",width:"1602",height:"271"})}),"\n",(0,i.jsx)(n.p,{children:"I asked Mistral to explain RAG to me like I'm 5, this was the response (after initially confusing the acronym):"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:">>> now do the 5 year old version of the correct acronym now\n Sure thing! Imagine there are four special friends named Retrieval, Augmentation, Generation, and Architect.\n\nRetrieval is the memory friend who keeps all the information in a computer program. He looks up answers to questions and finds important facts when needed.\n\nAugmentation is the helper friend who makes things better for the other friends. In our case, it adds extra details and information to make answers more complete and accurate.\n\nGeneration is the storyteller friend who turns information into answers that we can understand. He takes all the facts and turns them into clear and easy-to-understand responses.\n\nArchitect is the designer friend who puts everything together. He makes sure that all the friends work well together to give us the best possible answers!\n\nSo, as a 5-year-old, you could think of Retrieval-Augmented Generation (RAG) as a group of special friends named Retrieval, Augmentation, and Generation who work together to find information, add extra details, and \nturn it all into clear answers for us!\n\n>>> Send a message (/? for help)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"resources",children:"Resources"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://www.youtube.com/watch?v=aSh66tG1B5o&ab_channel=FahdMirza",children:'"Install and Run Mistral 7B on AWS"'})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},47870:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/Mistral7B-JupyterLab-Space-Notebook-8642203f7665fac04d2564cb25549cb9.png"},62576:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/Mistral7B-SageMaker-Studio-ed57a4f864a45fb5c4adca7c8dbecc32.png"},62911:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/Mistral7B-JupyterLab-46fab3e2ce256d1e9e7858ddba690b5d.png"},83262:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/Mistral7B-JupyterLab-CreateSpace-f09851f9baf7116bea023641bbc7e8e2.png"}}]);