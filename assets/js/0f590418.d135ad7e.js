"use strict";(self.webpackChunkronamosa_github_io=self.webpackChunkronamosa_github_io||[]).push([[83644],{26575:(n,e,t)=>{t.d(e,{A:()=>i});const i=t.p+"assets/images/pgptaws-header-708fd3b35f5524cedff98ab2f1bae317.png"},28453:(n,e,t)=>{t.d(e,{R:()=>a,x:()=>o});var i=t(96540);const l={},s=i.createContext(l);function a(n){const e=i.useContext(s);return i.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(l):n.components||l:a(n.components),i.createElement(s.Provider,{value:e},n.children)}},47526:(n,e,t)=>{t.d(e,{A:()=>i});const i=t.p+"assets/images/pgptaws-qdocs-09326ce10c4cc31ab4c584607d9352f6.png"},59087:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>r,contentTitle:()=>o,default:()=>g,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"engineer/AI/PrivateGPTAWS","title":"PrivateGPT on AWS EC2 - Secure Cloud-Based Document Chat with LLMs","description":"Deploy PrivateGPT on AWS EC2 for secure, scalable document chat in the cloud. Complete setup guide for private LLM deployment with document processing capabilities.","source":"@site/docs/engineer/AI/PrivateGPTAWS.md","sourceDirName":"engineer/AI","slug":"/engineer/AI/PrivateGPTAWS","permalink":"/docs/engineer/AI/PrivateGPTAWS","draft":false,"unlisted":false,"editUrl":"https://github.com/ronamosa/ronamosa.github.io/edit/main/website/docs/engineer/AI/PrivateGPTAWS.md","tags":[{"inline":true,"label":"ai","permalink":"/docs/tags/ai"},{"inline":true,"label":"aws","permalink":"/docs/tags/aws"},{"inline":true,"label":"llm","permalink":"/docs/tags/llm"},{"inline":true,"label":"ec2","permalink":"/docs/tags/ec-2"},{"inline":true,"label":"document-processing","permalink":"/docs/tags/document-processing"}],"version":"current","lastUpdatedBy":"R. Amosa","lastUpdatedAt":1705708800000,"sidebarPosition":2,"frontMatter":{"title":"PrivateGPT on AWS EC2 - Secure Cloud-Based Document Chat with LLMs","description":"Deploy PrivateGPT on AWS EC2 for secure, scalable document chat in the cloud. Complete setup guide for private LLM deployment with document processing capabilities.","keywords":["privategpt aws","aws ec2 llm","cloud llm deployment","private gpt aws","document chat aws","llm ec2","aws ai deployment"],"tags":["ai","aws","llm","ec2","document-processing"],"hide_title":false,"hide_table_of_contents":false,"sidebar_position":2,"last_update":{"date":"01/20/2024","author":"R. Amosa"}},"sidebar":"docsSidebar","previous":{"title":"PrivateGPT Local Deployment on Linux - Secure Document Chat Without Cloud","permalink":"/docs/engineer/AI/PrivateGPT"},"next":{"title":"Building Agentic RAG with LlamaIndex - Advanced AI Agent Development","permalink":"/docs/engineer/AI/AgenticRagLlamaIndex"}}');var l=t(74848),s=t(28453);const a={title:"PrivateGPT on AWS EC2 - Secure Cloud-Based Document Chat with LLMs",description:"Deploy PrivateGPT on AWS EC2 for secure, scalable document chat in the cloud. Complete setup guide for private LLM deployment with document processing capabilities.",keywords:["privategpt aws","aws ec2 llm","cloud llm deployment","private gpt aws","document chat aws","llm ec2","aws ai deployment"],tags:["ai","aws","llm","ec2","document-processing"],hide_title:!1,hide_table_of_contents:!1,sidebar_position:2,last_update:{date:"01/20/2024",author:"R. Amosa"}},o=void 0,r={},c=[{value:"Requirements",id:"requirements",level:2},{value:"Amazon Web Services (AWS)",id:"amazon-web-services-aws",level:2},{value:"Instance Type",id:"instance-type",level:3},{value:"Security Groups",id:"security-groups",level:3},{value:"Installation",id:"installation",level:2},{value:"pyenv",id:"pyenv",level:3},{value:"Poetry",id:"poetry",level:3},{value:"Runtime",id:"runtime",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Solution",id:"solution",level:3},{value:"Appendix",id:"appendix",level:2},{value:"GPU or no GPU?",id:"gpu-or-no-gpu",level:4}];function d(n){const e={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...n.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(e.p,{children:(0,l.jsx)(e.img,{alt:"header",src:t(26575).A+"",width:"2572",height:"1108"})}),"\n",(0,l.jsx)(e.admonition,{type:"info",children:(0,l.jsxs)(e.p,{children:["I setup PrivateGPT in previous ",(0,l.jsx)(e.a,{href:"/docs/engineer/AI/PrivateGPT",children:"post"})," locally on Proxmox. Now setting it up on Cloud/AWS to access more compute power."]})}),"\n",(0,l.jsx)(e.h2,{id:"requirements",children:"Requirements"}),"\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsxs)(e.li,{children:["PrivateGPT: ",(0,l.jsx)(e.a,{href:"https://github.com/imartinez/privateGPT.git",children:"https://github.com/imartinez/privateGPT.git"}),"\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsxs)(e.li,{children:["Installation Docs: ",(0,l.jsx)(e.a,{href:"https://docs.privategpt.dev/installation",children:"https://docs.privategpt.dev/installation"})]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(e.li,{children:["Poetry: ",(0,l.jsx)(e.a,{href:"https://python-poetry.org/docs/#installation",children:"https://python-poetry.org/docs/#installation"})]}),"\n"]}),"\n",(0,l.jsx)(e.h2,{id:"amazon-web-services-aws",children:"Amazon Web Services (AWS)"}),"\n",(0,l.jsx)(e.p,{children:"I use my own AWS account to launch the EC2 instance where I will install PrivateGPT."}),"\n",(0,l.jsx)(e.h3,{id:"instance-type",children:"Instance Type"}),"\n",(0,l.jsxs)(e.p,{children:["I am using a ",(0,l.jsx)(e.code,{children:"g4dn.4xlarge"})," EC2 instance."]}),"\n",(0,l.jsxs)(e.table,{children:[(0,l.jsx)(e.thead,{children:(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.th,{children:"Instance Type"}),(0,l.jsx)(e.th,{children:"vCPUs"}),(0,l.jsx)(e.th,{children:"Architecture"}),(0,l.jsx)(e.th,{children:"Memory (GiB)"}),(0,l.jsx)(e.th,{children:"Instance Storage (GB)"}),(0,l.jsx)(e.th,{children:"Storage type"}),(0,l.jsx)(e.th,{children:"Network Bandwidth (Gbps)"}),(0,l.jsx)(e.th,{children:"On-Demand Price/hr*"})]})}),(0,l.jsx)(e.tbody,{children:(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:(0,l.jsx)(e.code,{children:"g4dn.4xlarge"})}),(0,l.jsx)(e.td,{children:"16"}),(0,l.jsx)(e.td,{children:"x86_64"}),(0,l.jsx)(e.td,{children:"64"}),(0,l.jsx)(e.td,{children:"225"}),(0,l.jsx)(e.td,{children:"SSD"}),(0,l.jsx)(e.td,{children:"Up to 25Gigabit"}),(0,l.jsx)(e.td,{children:"1.566 USD per Hour"})]})})]}),"\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsx)(e.li,{children:"AMI ID= ami-04f5097681773b989"}),"\n",(0,l.jsx)(e.li,{children:"Ubuntu Server 22.04 LTS (HVM), SSD Volume Type."}),"\n",(0,l.jsx)(e.li,{children:'Description= "Canonical, Ubuntu, 22.04 LTS, amd64 jammy image build on 2023-12-07"'}),"\n"]}),"\n",(0,l.jsx)(e.h3,{id:"security-groups",children:"Security Groups"}),"\n",(0,l.jsx)(e.p,{children:"The bare minimum security to set on here is to lock down the following ports to just my IP address on the inbound security group rules."}),"\n",(0,l.jsx)(e.p,{children:"Ports: 22, 80, 443, 8001 (privateGPT)"}),"\n",(0,l.jsx)(e.p,{children:(0,l.jsx)(e.img,{alt:"sg rules",src:t(76258).A+"",width:"1336",height:"324"})}),"\n",(0,l.jsx)(e.h2,{id:"installation",children:"Installation"}),"\n",(0,l.jsx)(e.p,{children:"Launch EC2 instance via AWS console, use SSH Keypair to SSH from local terminal to EC2 instance."}),"\n",(0,l.jsx)(e.p,{children:(0,l.jsx)(e.img,{alt:"ssh ec2",src:t(74748).A+"",width:"1866",height:"986"})}),"\n",(0,l.jsx)(e.p,{children:"Then go ahead with the following commands:"}),"\n",(0,l.jsx)(e.h3,{id:"pyenv",children:"pyenv"}),"\n",(0,l.jsx)(e.p,{children:"Clone PrivateGPT repo:"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-bash",children:"git clone https://github.com/imartinez/privateGPT\ncd privateGPT\n"})}),"\n",(0,l.jsx)(e.p,{children:"Install pyenv, and environment setup:"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-bash",children:'# install script pyenv\ncurl https://pyenv.run | bash\n\n# add to ~/.bashrc\n# Pyenv\nexport PYENV_ROOT="$HOME/.pyenv"\n[[ -d $PYENV_ROOT/bin ]] && export PATH="$PYENV_ROOT/bin:$PATH"\neval "$(pyenv init -)"\n\n# reload shell\nsource ~/.bashrc\n\n# then add this line, and reload shell again\neval "$(pyenv virtualenv-init -)"\n'})}),"\n",(0,l.jsx)(e.p,{children:"Install build dependencies"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-bash",children:"sudo apt-get update\nsudo apt-get install -y make build-essential libssl-dev zlib1g-dev libbz2-dev \\\n    libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev libncursesw5-dev \\\n    xz-utils tk-dev libffi-dev liblzma-dev git\n"})}),"\n",(0,l.jsx)(e.p,{children:"Install python 3.11 in our pyenv"}),"\n",(0,l.jsxs)(e.admonition,{type:"caution",children:[(0,l.jsxs)(e.p,{children:["AWS: when using the ",(0,l.jsx)(e.code,{children:"Ubuntu Server Pro, 22.04 LTS, amd64 jammy image build on 2023-12-07 ami-0ac438f9a63fdd525"})," OS, I had a lot of issues with the ",(0,l.jsx)(e.code,{children:"pyenv install 3.11"})," command."]}),(0,l.jsxs)(e.p,{children:["Solution: go with the ",(0,l.jsx)(e.code,{children:"Ubuntu, 22.04 LTS, amd64 jammy image build on 2023-12-07 ami-04f5097681773b989"})," Free tier AMI."]})]}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-bash",children:"ubuntu@ip-xxx-xxx-xxx-xxx:~/privateGPT$ pyenv install 3.11\nDownloading Python-3.11.7.tar.xz...\n-> https://www.python.org/ftp/python/3.11.7/Python-3.11.7.tar.xz\nInstalling Python-3.11.7...\nInstalled Python-3.11.7 to /home/ubuntu/.pyenv/versions/3.11.7\n"})}),"\n",(0,l.jsxs)(e.p,{children:["Now ",(0,l.jsx)(e.code,{children:"pyenv local 3.11"})," to set local version."]}),"\n",(0,l.jsx)(e.h3,{id:"poetry",children:"Poetry"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-bash",children:"curl -sSL https://install.python-poetry.org | python3 -\n"})}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-bash",children:'# add to ~/.bashrc\nexport PATH="/home/ubuntu/.local/bin:$PATH"\n'})}),"\n",(0,l.jsxs)(e.p,{children:["test poetry ",(0,l.jsx)(e.code,{children:"poetry --version"})]}),"\n",(0,l.jsxs)(e.p,{children:["install UI: ",(0,l.jsx)(e.code,{children:"poetry install --with ui"})]}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-bash",children:"ubuntu@ip-xxx-xxx-xxx-xxx:~/privateGPT$ poetry install --with ui\nCreating virtualenv private-gpt-Wtvj2B-w-py3.11 in /home/ubuntu/.cache/pypoetry/virtualenvs\nInstalling dependencies from lock file\n\nPackage operations: 168 installs, 0 updates, 0 removals\n\n  \u2022 Installing frozenlist (1.4.1)\n  \u2022 Installing idna (3.6)\n  \u2022 Installing multidict (6.0.4)\n  \u2022 Installing aiosignal (1.3.1)\n  \u2022 Installing attrs (23.1.0)\n  \u2022 Installing certifi (2023.11.17)\n  \u2022 Installing charset-normalizer (3.3.2)\n  \u2022 Installing nvidia-nvjitlink-cu12 (12.3.101)\n  \u2022 Installing six (1.16.0)\n  \u2022 Installing urllib3 (1.26.18)\n  \u2022 Installing wrapt (1.16.0)\n  \u2022 Installing yarl (1.9.4)\n  \u2022 Installing aiohttp (3.9.1): Pending...\n  \u2022 Installing deprecated (1.2.14)\n  \u2022 Installing dill (0.3.7)\n  \u2022 Installing filelock (3.13.1)\n  \u2022 Installing fsspec (2023.12.2)\n  \u2022 Installing aiohttp (3.9.1): Downloading... 0%\n  \u2022 Installing deprecated (1.2.14)\n  \u2022 Installing dill (0.3.7)\n  \u2022 Installing filelock (3.13.1)\n  \u2022 Installing aiohttp (3.9.1)\n  \u2022 Installing deprecated (1.2.14)\n  \u2022 Installing dill (0.3.7)\n  \u2022 Installing filelock (3.13.1)\n  \u2022 Installing fsspec (2023.12.2)\n  \u2022 Installing markupsafe (2.1.3)\n  \u2022 Installing mpmath (1.3.0)\n  \u2022 Installing numpy (1.26.0)\n  \u2022 Installing nvidia-cublas-cu12 (12.1.3.1): Downloading... 40%\n  \u2022 Installing nvidia-cublas-cu12 (12.1.3.1): Installing...\n  \u2022 Installing nvidia-cublas-cu12 (12.1.3.1)\n  \u2022 Installing nvidia-cusparse-cu12 (12.1.0.106)\n  \u2022 Installing packaging (23.2)\n  \u2022 Installing python-dateutil (2.8.2)\n  \u2022 Installing pytz (2023.3.post1)\n  \u2022 Installing pyyaml (6.0.1)\n  \u2022 Installing requests (2.31.0)\n  \u2022 Installing rpds-py (0.14.1)\n  \u2022 Installing tqdm (4.66.1)\n  \u2022 Installing typing-extensions (4.9.0)\n  \u2022 Installing tzdata (2023.3)\n  \u2022 Installing h11 (0.14.0)\n  \u2022 Installing huggingface-hub (0.19.4)\n  \u2022 Installing humanfriendly (10.0)\n  \u2022 Installing jinja2 (3.1.2)\n  \u2022 Installing mdurl (0.1.2)\n  \u2022 Installing multiprocess (0.70.15)\n  \u2022 Installing networkx (3.2.1)\n  \u2022 Installing nvidia-cuda-cupti-cu12 (12.1.105): Downloading... 70%\n  \u2022 Installing nvidia-cuda-nvrtc-cu12 (12.1.105): Downloading... 40%\n  \u2022 Installing nvidia-cuda-runtime-cu12 (12.1.105): Pending...\n  \u2022 Installing nvidia-cuda-cupti-cu12 (12.1.105): Downloading... 80%\n  \u2022 Installing nvidia-cuda-nvrtc-cu12 (12.1.105): Downloading... 40%\n  \u2022 Installing nvidia-cuda-runtime-cu12 (12.1.105): Downloading... 0%\n  \u2022 Installing nvidia-cudnn-cu12 (8.9.2.26): Downloading... 1%\n  \u2022 Installing nvidia-cufft-cu12 (11.0.2.54): Downloading... 0%\n  \u2022 Installing nvidia-curand-cu12 (10.3.2.106): Pending...\n  \u2022 Installing nvidia-cusolver-cu12 (11.4.5.107): Pending...\n  \u2022 Installing nvidia-nccl-cu12 (2.18.1): Pending...\n  \u2022 Installing nvidia-cuda-cupti-cu12 (12.1.105): Installing...\n  \u2022 Installing nvidia-cuda-nvrtc-cu12 (12.1.105): Downloading... 60%\n  \u2022 Installing nvidia-cuda-cupti-cu12 (12.1.105)\n  \u2022 Installing nvidia-cuda-nvrtc-cu12 (12.1.105): Installing...\n  \u2022 Installing nvidia-cuda-nvrtc-cu12 (12.1.105)\n  \u2022 Installing nvidia-cuda-runtime-cu12 (12.1.105)\n  \u2022 Installing nvidia-cudnn-cu12 (8.9.2.26): Downloading... 11%\n  \u2022 Installing nvidia-cudnn-cu12 (8.9.2.26): Downloading... 22%\n  \u2022 Installing nvidia-cufft-cu12 (11.0.2.54): Installing...\n  \u2022 Installing nvidia-cudnn-cu12 (8.9.2.26): Downloading... 39%\n  \u2022 Installing nvidia-cudnn-cu12 (8.9.2.26): Installing...\n  \u2022 Installing nvidia-cudnn-cu12 (8.9.2.26)\n  \u2022 Installing nvidia-cufft-cu12 (11.0.2.54)\n  \u2022 Installing nvidia-curand-cu12 (10.3.2.106)\n  \u2022 Installing nvidia-cusolver-cu12 (11.4.5.107)\n  \u2022 Installing nvidia-nccl-cu12 (2.18.1)\n  \u2022 Installing nvidia-nvtx-cu12 (12.1.105)\n  \u2022 Installing pandas (2.1.4)\n  \u2022 Installing protobuf (4.25.1)\n  \u2022 Installing pyarrow (14.0.1)\n  \u2022 Installing referencing (0.32.0)\n  \u2022 Installing sniffio (1.3.0)\n  \u2022 Installing sympy (1.12)\n  \u2022 Installing triton (2.1.0)\n  \u2022 Installing xxhash (3.4.1)\n  \u2022 Installing annotated-types (0.6.0)\n  \u2022 Installing anyio (3.7.1)\n  \u2022 Installing coloredlogs (15.0.1)\n  \u2022 Installing datasets (2.14.4)\n  \u2022 Installing flatbuffers (23.5.26)\n  \u2022 Installing hpack (4.0.0)\n  \u2022 Installing httpcore (1.0.2)\n  \u2022 Installing hyperframe (6.0.1)\n  \u2022 Installing jmespath (1.0.1)\n  \u2022 Installing jsonschema-specifications (2023.11.2)\n  \u2022 Installing markdown-it-py (3.0.0): Pending...\n  \u2022 Installing mypy-extensions (1.0.0): Pending...\n  \u2022 Installing markdown-it-py (3.0.0): Installing...\n  \u2022 Installing mypy-extensions (1.0.0)\n  \u2022 Installing markdown-it-py (3.0.0)\n  \u2022 Installing mypy-extensions (1.0.0)\n  \u2022 Installing psutil (5.9.6)\n  \u2022 Installing pydantic-core (2.14.5)\n  \u2022 Installing pygments (2.17.2): Installing...\n  \u2022 Installing regex (2023.10.3): Pending...\n  \u2022 Installing responses (0.18.0): Pending...\n  \u2022 Installing safetensors (0.4.1): Pending...\n  \u2022 Installing sentencepiece (0.1.99): Pending...\n  \u2022 Installing pygments (2.17.2)\n  \u2022 Installing regex (2023.10.3)\n  \u2022 Installing responses (0.18.0)\n  \u2022 Installing safetensors (0.4.1)\n  \u2022 Installing sentencepiece (0.1.99)\n  \u2022 Installing tokenizers (0.15.0)\n  \u2022 Installing torch (2.1.2)\n  \u2022 Installing accelerate (0.25.0)\n  \u2022 Installing botocore (1.34.2): Downloading... 90%\n  \u2022 Installing click (8.1.7)\n  \u2022 Installing colorama (0.4.6)\n  \u2022 Installing contourpy (1.2.0)\n  \u2022 Installing botocore (1.34.2): Downloading... 100%\n  \u2022 Installing click (8.1.7)\n  \u2022 Installing colorama (0.4.6)\n  \u2022 Installing botocore (1.34.2): Installing...\n  \u2022 Installing botocore (1.34.2)\n  \u2022 Installing click (8.1.7)\n  \u2022 Installing colorama (0.4.6)\n  \u2022 Installing contourpy (1.2.0)\n  \u2022 Installing cycler (0.12.1)\n  \u2022 Installing distlib (0.3.8)\n  \u2022 Installing distro (1.8.0)\n  \u2022 Installing dnspython (2.4.2)\n  \u2022 Installing evaluate (0.4.1)\n  \u2022 Installing fonttools (4.46.0)\n  \u2022 Installing greenlet (3.0.2)\n  \u2022 Installing grpcio (1.60.0)\n  \u2022 Installing h2 (4.1.0)\n  \u2022 Installing httptools (0.6.1)\n  \u2022 Installing httpx (0.25.2)\n  \u2022 Installing iniconfig (2.0.0)\n  \u2022 Installing joblib (1.3.2)\n  \u2022 Installing jsonschema (4.20.0)\n  \u2022 Installing kiwisolver (1.4.5)\n  \u2022 Installing marshmallow (3.20.1)\n  \u2022 Installing onnx (1.15.0)\n  \u2022 Installing onnxruntime (1.16.3)\n  \u2022 Installing pillow (10.1.0)\n  \u2022 Installing platformdirs (4.1.0)\n  \u2022 Installing pluggy (1.3.0)\n  \u2022 Installing pydantic (2.5.2)\n  \u2022 Installing pyparsing (3.1.1)\n  \u2022 Installing python-dotenv (1.0.0)\n  \u2022 Installing rich (13.7.0)\n  \u2022 Installing shellingham (1.5.4)\n  \u2022 Installing soupsieve (2.5)\n  \u2022 Installing starlette (0.27.0)\n  \u2022 Installing toolz (0.12.0)\n  \u2022 Installing transformers (4.36.1)\n  \u2022 Installing typing-inspect (0.9.0)\n  \u2022 Installing uvloop (0.19.0)\n  \u2022 Installing watchfiles (0.21.0)\n  \u2022 Installing websockets (11.0.3)\n  \u2022 Installing aiofiles (23.2.1)\n  \u2022 Installing aiostream (0.5.2)\n  \u2022 Installing altair (5.2.0)\n  \u2022 Installing beautifulsoup4 (4.12.2)\n  \u2022 Installing cfgv (3.4.0)\n  \u2022 Installing coverage (7.3.3)\n  \u2022 Installing dataclasses-json (0.5.14)\n  \u2022 Installing email-validator (2.1.0.post1)\n  \u2022 Installing fastapi (0.103.2)\n  \u2022 Installing ffmpy (0.3.1): Pending...\n  \u2022 Installing gradio-client (0.7.3)\n  \u2022 Installing grpcio-tools (1.60.0): Pending...\n  \u2022 Installing identify (2.5.33): Pending...\n  \u2022 Installing importlib-resources (6.1.1): Pending...\n  \u2022 Installing itsdangerous (2.1.2): Pending...\n  \u2022 Installing importlib-resources (6.1.1)\n  \u2022 Installing itsdangerous (2.1.2)\n  \u2022 Installing ffmpy (0.3.1): Downloading... 0%\n  \u2022 Installing gradio-client (0.7.3)\n  \u2022 Installing ffmpy (0.3.1)\n  \u2022 Installing gradio-client (0.7.3)\n  \u2022 Installing grpcio-tools (1.60.0)\n  \u2022 Installing identify (2.5.33)\n  \u2022 Installing importlib-resources (6.1.1)\n  \u2022 Installing itsdangerous (2.1.2)\n  \u2022 Installing matplotlib (3.8.2)\n  \u2022 Installing nest-asyncio (1.5.8)\n  \u2022 Installing nltk (3.8.1)\n  \u2022 Installing nodeenv (1.8.0)\n  \u2022 Installing openai (1.5.0)\n  \u2022 Installing optimum (1.16.1)\n  \u2022 Installing orjson (3.9.10)\n  \u2022 Installing pathspec (0.12.1)\n  \u2022 Installing portalocker (2.8.2)\n  \u2022 Installing pydantic-extra-types (2.2.0)\n  \u2022 Installing pydantic-settings (2.1.0)\n  \u2022 Installing pydub (0.25.1)\n  \u2022 Installing pytest (7.4.3)\n  \u2022 Installing python-multipart (0.0.6)\n  \u2022 Installing s3transfer (0.9.0)\n  \u2022 Installing semantic-version (2.10.0)\n  \u2022 Installing sqlalchemy (2.0.23)\n  \u2022 Installing tenacity (8.2.3)\n  \u2022 Installing tiktoken (0.5.2)\n  \u2022 Installing tomlkit (0.12.0)\n  \u2022 Installing typer (0.9.0)\n  \u2022 Installing ujson (5.9.0)\n  \u2022 Installing uvicorn (0.24.0.post1)\n  \u2022 Installing virtualenv (20.25.0)\n  \u2022 Installing black (22.12.0): Installing...\n  \u2022 Installing black (22.12.0)\n  \u2022 Installing boto3 (1.34.2)\n  \u2022 Installing gradio (4.10.0): Downloading... 50%\n  \u2022 Installing injector (0.21.0)\n  \u2022 Installing llama-index (0.9.3)\n  \u2022 Installing gradio (4.10.0)\n  \u2022 Installing injector (0.21.0)\n  \u2022 Installing llama-index (0.9.3)\n  \u2022 Installing mypy (1.7.1)\n  \u2022 Installing pre-commit (2.21.0)\n  \u2022 Installing pypdf (3.17.2)\n  \u2022 Installing pytest-asyncio (0.21.1)\n  \u2022 Installing pytest-cov (3.0.0)\n  \u2022 Installing qdrant-client (1.7.0)\n  \u2022 Installing ruff (0.1.8)\n  \u2022 Installing types-pyyaml (6.0.12.12)\n  \u2022 Installing watchdog (3.0.0)\n\nInstalling the current project: private-gpt (0.2.0)\nubuntu@ip-xxx-xxx-xxx-xxx:~/privateGPT$\n"})}),"\n",(0,l.jsxs)(e.p,{children:["install poetry with local: ",(0,l.jsx)(e.code,{children:"poetry install --with local"})]}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-bash",children:"ubuntu@ip-xxx-xxx-xxx-xxx:~/privateGPT$ poetry install --with local\nInstalling dependencies from lock file\n\nPackage operations: 7 installs, 0 updates, 0 removals\n\n  \u2022 Installing scipy (1.11.4)\n  \u2022 Installing threadpoolctl (3.2.0)\n  \u2022 Installing diskcache (5.6.3)\n  \u2022 Installing scikit-learn (1.3.2)\n  \u2022 Installing torchvision (0.16.2)\n  \u2022 Installing llama-cpp-python (0.2.23)\n  \u2022 Installing sentence-transformers (2.2.2)\n\nInstalling the current project: private-gpt (0.2.0)\n"})}),"\n",(0,l.jsx)(e.h2,{id:"runtime",children:"Runtime"}),"\n",(0,l.jsxs)(e.p,{children:["Run setup: ",(0,l.jsx)(e.code,{children:"poetry run python scripts/setup"})]}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-bash",children:"ubuntu@ip-xxx-xxx-xxx-xxx:~/privateGPT$ poetry run python scripts/setup\n00:16:27.904 [INFO    ] private_gpt.settings.settings_loader - Starting application with profiles=['default']\nDownloading embedding BAAI/bge-small-en-v1.5\nconfig_sentence_transformers.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 124/124 [00:00<00:00, 1.03MB/s]\nmodules.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 349/349 [00:00<00:00, 3.44MB/s]\n.gitattributes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.52k/1.52k [00:00<00:00, 14.5MB/s]\nREADME.md: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 90.3k/90.3k [00:00<00:00, 453kB/s]\nconfig.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 743/743 [00:00<00:00, 6.94MB/s]\n1_Pooling/config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 190/190 [00:00<00:00, 2.03MB/s]\nsentence_bert_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 52.0/52.0 [00:00<00:00, 601kB/s]\ntokenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 366/366 [00:00<00:00, 3.47MB/s]\nspecial_tokens_map.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 125/125 [00:00<00:00, 1.22MB/s]\nvocab.txt: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 232k/232k [00:00<00:00, 592kB/s]\ntokenizer.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 711k/711k [00:00<00:00, 912kB/s]\nmodel.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133M/133M [00:09<00:00, 14.4MB/s]\npytorch_model.bin: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 134M/134M [00:10<00:00, 13.2MB/s]\nFetching 13 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:12<00:00,  1.02it/s]\nEmbedding model downloaded!\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                          | 105M/134M [00:08<00:01, 15.9MB/s]\nDownloading LLM mistral-7b-instruct-v0.2.Q4_K_M.gguf\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 134M/134M [00:10<00:00, 15.7MB/s]\nmistral-7b-instruct-v0.2.Q4_K_M.gguf: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.37G/4.37G [00:09<00:00, 451MB/s]\nLLM model downloaded!\nDownloading tokenizer mistralai/Mistral-7B-Instruct-v0.2\ntokenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.46k/1.46k [00:00<00:00, 12.1MB/s]\ntokenizer.model: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 493k/493k [00:00<00:00, 369MB/s]\ntokenizer.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.80M/1.80M [00:00<00:00, 3.04MB/s]\nspecial_tokens_map.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 72.0/72.0 [00:00<00:00, 770kB/s]\nTokenizer downloaded!\nSetup done\nubuntu@ip-xxx-xxx-xxx-xxx:~/privateGPT$\n"})}),"\n",(0,l.jsx)(e.p,{children:"Launch PrivateGPT API and start the UI."}),"\n",(0,l.jsx)(e.p,{children:"Because we've gone with poetry for dependencies, we launch PrivateGPT with poetry"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-bash",children:'poetry run python -m private_gpt\n\nubuntu@ip-172-31-3-217:~/privateGPT$ poetry run python -m private_gpt\n00:55:51.178 [INFO    ] private_gpt.settings.settings_loader - Starting application with profiles=[\'default\']\n00:55:56.198 [INFO    ] private_gpt.components.llm.llm_component - Initializing the LLM in mode=local\nllama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /home/ubuntu/privateGPT/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\nllama_model_loader: - tensor    0:                token_embd.weight q4_K     [  4096, 32000,     1,     1 ]\nllama_model_loader: - tensor    1:              blk.0.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor    2:              blk.0.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor    3:              blk.0.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor    4:         blk.0.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n...\n...\n\nllama_new_context_with_model: compute buffer total size = 278.68 MiB\nAVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 |\n00:55:57.827 [INFO    ] private_gpt.components.embedding.embedding_component - Initializing the embedding model in mode=local\n00:55:59.757 [INFO    ] llama_index.indices.loading - Loading all indices.\n00:55:59.975 [INFO    ]         private_gpt.ui.ui - Mounting the gradio UI, at path=/\n00:56:00.029 [INFO    ]             uvicorn.error - Started server process [5078]\n00:56:00.029 [INFO    ]             uvicorn.error - Waiting for application startup.\n00:56:00.029 [INFO    ]             uvicorn.error - Application startup complete.\n00:56:00.030 [INFO    ]             uvicorn.error - Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)\n00:56:26.108 [INFO    ]            uvicorn.access - 111.111.111.111:62170 - "GET / HTTP/1.1" 200\n00:56:26.961 [INFO    ]            uvicorn.access - 111.111.111.111:62170 - "GET /info HTTP/1.1" 200\n00:56:27.004 [INFO    ]            uvicorn.access - 111.111.111.111:62170 - "GET /theme.css HTTP/1.1" 200\n00:56:27.191 [INFO    ]            uvicorn.access - 111.111.111.111:62170 - "POST /run/predict HTTP/1.1" 200\n00:56:27.243 [INFO    ]            uvicorn.access - 111.111.111.111:62170 - "POST /queue/join HTTP/1.1" 200\n00:56:27.282 [INFO    ]            uvicorn.access - 111.111.111.111:62170 - "GET /queue/data?session_hash=2w7ysm4962n HTTP/1.1" 200\n00:56:31.184 [INFO    ]            uvicorn.access - 111.111.111.111:62172 - "POST /queue/join HTTP/1.1" 200\n00:56:31.217 [INFO    ]         private_gpt.ui.ui - Setting system prompt to: You are a helpful, respectful and honest assistant.  Always answer as helpfully as possible and follow ALL given instructions. Do not speculate or make up information. Do not reference any given instructions or context.\n\n00:56:31.228 [INFO    ]            uvicorn.access - 111.111.111.111:62172 - "GET /queue/data?session_hash=2w7ysm4962n HTTP/1.1" 200\n00:56:33.600 [INFO    ]            uvicorn.access - 111.111.111.111:62172 - "POST /run/predict HTTP/1.1" 200\n00:56:33.636 [INFO    ]            uvicorn.access - 111.111.111.111:62173 - "POST /run/predict HTTP/1.1" 200\n00:56:33.681 [INFO    ]            uvicorn.access - 111.111.111.111:62173 - "POST /run/predict HTTP/1.1" 200\n00:56:33.736 [INFO    ]            uvicorn.access - 111.111.111.111:62173 - "POST /queue/join HTTP/1.1" 200\n00:56:33.843 [INFO    ]            uvicorn.access - 111.111.111.111:62173 - "GET /queue/data?session_hash=2w7ysm4962n HTTP/1.1" 200\n00:56:38.475 [INFO    ]            uvicorn.access - 111.111.111.111:62174 - "GET /file%3D/tmp/gradio/533bd8ba49221de137dda94fdfeed4bebe7a7878/avatar-bot.ico HTTP/1.1" 200\n\nllama_print_timings:        load time =    4463.60 ms\nllama_print_timings:      sample time =      36.43 ms /    68 runs   (    0.54 ms per token,  1866.85 tokens per second)\nllama_print_timings: prompt eval time =    4458.40 ms /    61 tokens (   73.09 ms per token,    13.68 tokens per second)\nllama_print_timings:        eval time =    8243.30 ms /    67 runs   (  123.03 ms per token,     8.13 tokens per second)\nllama_print_timings:       total time =   18502.42 ms\n** Prompt: **\n<s />[INST] You are a helpful, respectful and honest assistant.  Always answer as helpfully as possible and follow ALL given instructions. Do not speculate or make up information. Do not reference any given instructions or context. [/INST]</s>[INST] hi [/INST]\n**************************************************\n** Completion: **\n Hello! How may I assist you today? I\'m here to help answer any questions you have to the best of my ability. Please keep in mind that I cannot provide speculative or made-up information, and must always follow all given instructions. Let me know if there is a specific topic or question you have in mind.\n**************************************************\n\n\n** Messages: **\nsystem: You are a helpful, respectful and honest assistant.  Always answer as helpfully as possible and follow ALL given instructions. Do not speculate or make up information. Do not reference any given instructions or context.\n\nuser: hi\n**************************************************\n** Response: **\nassistant:  Hello! How may I assist you today? I\'m here to help answer any questions you have to the best of my ability. Please keep in mind that I cannot provide speculative or made-up information, and must always follow all given instructions. Let me know if there is a specific topic or question you have in mind.\n**************************************************\n\n\n00:56:52.404 [INFO    ]            uvicorn.access - 111.111.111.111:62173 - "POST /run/predict HTTP/1.1" 200\n'})}),"\n",(0,l.jsx)(e.p,{children:"Success."}),"\n",(0,l.jsx)(e.p,{children:"Chat Function:"}),"\n",(0,l.jsx)(e.p,{children:(0,l.jsx)(e.img,{alt:"pgpt chat",src:t(60014).A+"",width:"2516",height:"960"})}),"\n",(0,l.jsx)(e.p,{children:"Query Document that has been uploaded:"}),"\n",(0,l.jsx)(e.admonition,{type:"note",children:(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsx)(e.li,{children:"ingestion process i.e. chunking and embedding the doc was noticable faster here than in my local privateGPT setup in previous post."}),"\n",(0,l.jsx)(e.li,{children:"the summary given here, imo, was not great (using Mistral7B model)."}),"\n"]})}),"\n",(0,l.jsx)(e.p,{children:(0,l.jsx)(e.img,{alt:"pgpt query",src:t(47526).A+"",width:"2526",height:"1520"})}),"\n",(0,l.jsx)(e.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,l.jsxs)(e.p,{children:['Errors: "',(0,l.jsx)(e.code,{children:"Collection make_this_parameterizable_per_api_call not found"}),'" on the UI.']}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-bash",children:'# from stdout\n...\nFile "/home/ubuntu/.cache/pypoetry/virtualenvs/private-gpt-Wtvj2B-w-py3.11/lib/python3.11/site-packages/qdrant_client/local/qdrant_local.py", line 121, in _get_collection\n    raise ValueError(f"Collection {collection_name} not found")\nValueError: Collection make_this_parameterizable_per_api_call not found\n...\n'})}),"\n",(0,l.jsx)(e.p,{children:"related and further down the stdout output:"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-bash",children:"...\nAttributeError: 'NoneType' object has no attribute 'split'\n...\n"})}),"\n",(0,l.jsxs)(e.p,{children:["I'm assuming the split is happening on a ",(0,l.jsx)(e.code,{children:"None"})," cos the collection can't be found further up."]}),"\n",(0,l.jsx)(e.h3,{id:"solution",children:"Solution"}),"\n",(0,l.jsx)(e.p,{children:"I restarted the process a few times trying to figure out what the problem was, gave same error couple of times, then magically after another restart it all started working again. smh."}),"\n",(0,l.jsx)(e.h2,{id:"appendix",children:"Appendix"}),"\n",(0,l.jsx)(e.p,{children:"Notes from looking at different components."}),"\n",(0,l.jsx)(e.p,{children:"From ChatGPT:"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-text",children:"For running Mistral 7xb, I would suggest starting with a p4d.24xlarge instance due to its superior computational capabilities and memory. However, if budget constraints are a concern, a g4dn.12xlarge instance can be a viable alternative. Always monitor the performance and adjust the specifications as needed.\n"})}),"\n",(0,l.jsx)(e.p,{children:"p4d.24xlarge specs ="}),"\n",(0,l.jsx)(e.p,{children:"g4dn.12xlarge specs ="}),"\n",(0,l.jsx)(e.p,{children:"region = ap-southeast-2"}),"\n",(0,l.jsx)(e.p,{children:"g5.12xlarge"}),"\n",(0,l.jsx)(e.h4,{id:"gpu-or-no-gpu",children:"GPU or no GPU?"}),"\n",(0,l.jsx)(e.p,{children:'It\'s the "inference" part of the LLM architecture that needs GPU for generation, but a high-powered CPU could be enough if your app is just processing requests out to the user.'}),"\n",(0,l.jsx)(e.p,{children:"ChatGPT had some interesting points to note:"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-text",children:"Model Size: Larger models with more parameters (like GPT-3's 175 billion parameters) require more computational power for inference. Smaller models might be efficiently run on CPUs or less powerful GPUs.\n\nInference Load: The number of queries your application processes simultaneously affects GPU requirements. High query volumes or the need for rapid responses can necessitate more powerful GPUs.\n\nComplexity of Tasks: More complex queries, such as those requiring understanding of longer contexts or generating lengthy responses, can increase computational demands.\n\nOptimization and Quantization: Models optimized for inference, potentially using techniques like quantization (reducing the precision of the model's parameters), can reduce GPU requirements.\n\nGPU vs CPU Inference: While GPUs are generally faster for model inference due to their parallel processing capabilities, some applications might run satisfactorily on modern CPUs, especially if inference demands are low.\n"})}),"\n",(0,l.jsx)(e.p,{children:"I had to request an increase in vCPU of G* EC2 instances for Region us-east-1, quota was zero, requested 48 vCPU"}),"\n",(0,l.jsx)(e.p,{children:"Created an EC2 Instance on AWS Cloud, running:"}),"\n",(0,l.jsxs)(e.table,{children:[(0,l.jsx)(e.thead,{children:(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.th,{children:"Instance Size"}),(0,l.jsx)(e.th,{children:"GPU"}),(0,l.jsx)(e.th,{children:"vCPUs"}),(0,l.jsx)(e.th,{children:"Memory (GiB)"}),(0,l.jsx)(e.th,{children:"Instance Storage (GB)"}),(0,l.jsx)(e.th,{children:"Network Bandwidth (Gbps)"}),(0,l.jsx)(e.th,{children:"EBS Bandwidth (Gbps)"}),(0,l.jsx)(e.th,{children:"On-Demand Price/hr*"})]})}),(0,l.jsx)(e.tbody,{children:(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:"g4dn.12xlarge"}),(0,l.jsx)(e.td,{children:"4"}),(0,l.jsx)(e.td,{children:"48"}),(0,l.jsx)(e.td,{children:"192"}),(0,l.jsx)(e.td,{children:"1 x 900 NVMe SSD"}),(0,l.jsx)(e.td,{children:"50"}),(0,l.jsx)(e.td,{children:"9.5"}),(0,l.jsx)(e.td,{children:"$3.192"})]})})]})]})}function g(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,l.jsx)(e,{...n,children:(0,l.jsx)(d,{...n})}):d(n)}},60014:(n,e,t)=>{t.d(e,{A:()=>i});const i=t.p+"assets/images/pgptaws-chat-8e997a8c00a7044cfd03efdbfe4f5aba.png"},74748:(n,e,t)=>{t.d(e,{A:()=>i});const i=t.p+"assets/images/pgptaws-ssh-2474c26e1b7f960011836fd36bd4e0c4.png"},76258:(n,e,t)=>{t.d(e,{A:()=>i});const i=t.p+"assets/images/pgptaws-sg-885f65c730f200bd86d51f600c8ad35c.png"}}]);