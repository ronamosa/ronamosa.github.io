"use strict";(self.webpackChunkronamosa_github_io=self.webpackChunkronamosa_github_io||[]).push([[64139],{25802:(e,n,o)=>{o.d(n,{A:()=>r});const r=o.p+"assets/images/SageMakerDeploy-endpoint-ba68531b44d5e983644750d7868ed14b.png"},28453:(e,n,o)=>{o.d(n,{R:()=>i,x:()=>s});var r=o(96540);const t={},a=r.createContext(t);function i(e){const n=r.useContext(a);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:i(e.components),r.createElement(a.Provider,{value:n},e.children)}},58490:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>i,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"engineer/AI/DeployLLMToSageMaker","title":"Deploy Large Language Models to AWS SageMaker - Complete Manual Guide","description":"Step-by-step tutorial for manually deploying LLMs to AWS SageMaker endpoints. Learn model preparation, inference code setup, and endpoint deployment for production AI.","source":"@site/docs/engineer/AI/DeployLLMToSageMaker.md","sourceDirName":"engineer/AI","slug":"/engineer/AI/deploy-llm-sagemaker-manual-guide","permalink":"/docs/engineer/AI/deploy-llm-sagemaker-manual-guide","draft":false,"unlisted":false,"editUrl":"https://github.com/ronamosa/ronamosa.github.io/edit/main/website/docs/engineer/AI/DeployLLMToSageMaker.md","tags":[{"inline":true,"label":"ai","permalink":"/docs/tags/ai"},{"inline":true,"label":"aws","permalink":"/docs/tags/aws"},{"inline":true,"label":"sagemaker","permalink":"/docs/tags/sagemaker"},{"inline":true,"label":"llm","permalink":"/docs/tags/llm"},{"inline":true,"label":"machine-learning","permalink":"/docs/tags/machine-learning"},{"inline":true,"label":"pytorch","permalink":"/docs/tags/pytorch"}],"version":"current","lastUpdatedBy":"Ron Amosa","lastUpdatedAt":1771617957000,"sidebarPosition":5,"frontMatter":{"title":"Deploy Large Language Models to AWS SageMaker - Complete Manual Guide","description":"Step-by-step tutorial for manually deploying LLMs to AWS SageMaker endpoints. Learn model preparation, inference code setup, and endpoint deployment for production AI.","keywords":["aws sagemaker","llm deployment","large language model","machine learning","ai deployment","pytorch","sagemaker endpoints","model serving"],"tags":["ai","aws","sagemaker","llm","machine-learning","pytorch"],"sidebar_position":5,"slug":"deploy-llm-sagemaker-manual-guide"},"sidebar":"docsSidebar","previous":{"title":"Building Agentic RAG with LlamaIndex - Advanced AI Agent Development","permalink":"/docs/engineer/AI/AgenticRagLlamaIndex"},"next":{"title":"AWS GPT Documentation Assistant - AI-Powered AWS Learning Tool","permalink":"/docs/engineer/AI/AWSGPT"}}');var t=o(74848),a=o(28453);const i={title:"Deploy Large Language Models to AWS SageMaker - Complete Manual Guide",description:"Step-by-step tutorial for manually deploying LLMs to AWS SageMaker endpoints. Learn model preparation, inference code setup, and endpoint deployment for production AI.",keywords:["aws sagemaker","llm deployment","large language model","machine learning","ai deployment","pytorch","sagemaker endpoints","model serving"],tags:["ai","aws","sagemaker","llm","machine-learning","pytorch"],sidebar_position:5,slug:"deploy-llm-sagemaker-manual-guide"},s=void 0,l={},d=[{value:"Agenda",id:"agenda",level:2},{value:"Deployment Overview",id:"deployment-overview",level:2},{value:"Prepare LLM",id:"prepare-llm",level:2},{value:"Model Theory",id:"model-theory",level:3},{value:"Example Stable Diffusion XL",id:"example-stable-diffusion-xl",level:3},{value:"Prepare LLM Model",id:"prepare-llm-model",level:2},{value:"Download LLM from HuggingFace",id:"download-llm-from-huggingface",level:3},{value:"Organise Model Artefacts",id:"organise-model-artefacts",level:3},{value:"Package Deployment Model",id:"package-deployment-model",level:3},{value:"Deploy Model to SageMaker",id:"deploy-model-to-sagemaker",level:2},{value:"Upload to s3",id:"upload-to-s3",level:3},{value:"Create Role",id:"create-role",level:3},{value:"Create Endpoint",id:"create-endpoint",level:3},{value:"Test the Endpoint",id:"test-the-endpoint",level:2},{value:"Invoke",id:"invoke",level:3},{value:"Bonus: Re-Deploy Updates to your SageMaker Endpoint",id:"bonus-re-deploy-updates-to-your-sagemaker-endpoint",level:2},{value:"Redeploy SageMaker Model",id:"redeploy-sagemaker-model",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"python3 deploy.py",id:"python3-deploypy",level:3},{value:"No such file or directory: &#39;inference.py&#39;",id:"no-such-file-or-directory-inferencepy",level:3},{value:"Invalid base64",id:"invalid-base64",level:3}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsx)(n.p,{children:"These are notes for working out the steps and process for deploying an LLM to SageMaker as part of my YouTube video series, hence they will be rough, but I will include the link to the completed video."})}),"\n",(0,t.jsx)(n.h2,{id:"agenda",children:"Agenda"}),"\n",(0,t.jsx)(n.p,{children:"In this first video, we'll focus on the end-to-end deployment process. We'll cover:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Preparing the LLM model for deployment"}),"\n",(0,t.jsx)(n.li,{children:"Writing the inference code to handle requests"}),"\n",(0,t.jsx)(n.li,{children:"Launching the model endpoint on SageMaker"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"deployment-overview",children:"Deployment Overview"}),"\n",(0,t.jsx)(n.p,{children:"Key components"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"model artefacts"}),"\n",(0,t.jsx)(n.li,{children:"inference code"}),"\n",(0,t.jsx)(n.li,{children:"Endpoint (sagemaker)"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Tasks at a high level"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"prepare the model files, upload to S3"}),"\n",(0,t.jsx)(n.li,{children:"write some inference code, package with deps"}),"\n",(0,t.jsx)(n.li,{children:"create SM model using the model package"}),"\n",(0,t.jsx)(n.li,{children:"launch Endpoint to servce model."}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prepare-llm",children:"Prepare LLM"}),"\n",(0,t.jsx)(n.h3,{id:"model-theory",children:"Model Theory"}),"\n",(0,t.jsx)(n.p,{children:"According to the SageMaker documentation, the model directory should have the following structure:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"/model.tar.gz\n\u251c\u2500\u2500 model.pth\n\u251c\u2500\u2500 code/\n\u2502   \u251c\u2500\u2500 inference.py\n\u2502   \u251c\u2500\u2500 requirements.txt\n"})}),"\n",(0,t.jsx)(n.p,{children:"Here's what each component represents:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"model.pth"}),": This is the serialized model file containing the trained weights of your PyTorch model."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"code/"}),": This directory contains the necessary code files for the inference script.","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"inference.py"}),": This is the inference script that we'll discuss later, containing the ",(0,t.jsx)(n.code,{children:"model_fn"}),", ",(0,t.jsx)(n.code,{children:"input_fn"}),", ",(0,t.jsx)(n.code,{children:"predict_fn"}),", and ",(0,t.jsx)(n.code,{children:"output_fn"})," functions."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"requirements.txt"}),": This file lists the dependencies required by your inference script, such as the ",(0,t.jsx)(n.code,{children:"torch"})," and ",(0,t.jsx)(n.code,{children:"transformers"})," libraries."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"example-stable-diffusion-xl",children:"Example Stable Diffusion XL"}),"\n",(0,t.jsx)(n.p,{children:"On my SageMaker instance, I deployed SDXL, from the terminal here's what the artefacts look like:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"sagemaker-user@default:~/model-58101$ ls -ll\ntotal 92\n-rw-r--r-- 1 sagemaker-user users 12506 Mar  7 04:03 README.md\ndrwxr-xr-x 3 sagemaker-user users    76 Mar  7 04:05 code\ndrwxr-xr-x 2 sagemaker-user users    38 Mar  7 04:04 feature_extractor\n-rw-r--r-- 1 sagemaker-user users   543 Mar  7 04:03 model_index.json\ndrwxr-xr-x 2 sagemaker-user users    50 Mar  7 04:04 safety_checker\ndrwxr-xr-x 3 sagemaker-user users    61 Mar  7 04:04 scheduler\ndrwxr-xr-x 2 sagemaker-user users    50 Mar  7 04:04 text_encoder\ndrwxr-xr-x 2 sagemaker-user users   102 Mar  7 04:04 tokenizer\ndrwxr-xr-x 2 sagemaker-user users    60 Mar  7 04:04 unet\n-rw-r--r-- 1 sagemaker-user users 71237 Mar  7 04:03 v1-variants-scores.jpg\ndrwxr-xr-x 2 sagemaker-user users    60 Mar  7 04:04 vae\nsagemaker-user@default:~/model-58101$\n"})}),"\n",(0,t.jsx)(n.p,{children:"looking in code:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"sagemaker-user@default:~/model-58101/code$ ls -ll\ntotal 8\n-rw-r--r-- 1 sagemaker-user users 1170 Mar  7 03:57 inference.py\n-rw-r--r-- 1 sagemaker-user users   38 Mar  7 03:57 requirements.txt\n"})}),"\n",(0,t.jsxs)(n.p,{children:["and the default ",(0,t.jsx)(n.code,{children:"inference.py"})," file that comes default"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import base64\nimport torch\nfrom io import BytesIO\nfrom diffusers import StableDiffusionPipeline\n\n\ndef model_fn(model_dir):\n    # Load stable diffusion and move it to the GPU\n    pipe = StableDiffusionPipeline.from_pretrained(model_dir, torch_dtype=torch.float16)\n    pipe = pipe.to("cuda")\n\n    return pipe\n\n\ndef predict_fn(data, pipe):\n\n    # get prompt & parameters\n    prompt = data.pop("inputs", data)\n    # set valid HP for stable diffusion\n    num_inference_steps = data.pop("num_inference_steps", 50)\n    guidance_scale = data.pop("guidance_scale", 7.5)\n    num_images_per_prompt = data.pop("num_images_per_prompt", 4)\n\n    # run generation with parameters\n    generated_images = pipe(\n        prompt,\n        num_inference_steps=num_inference_steps,\n        guidance_scale=guidance_scale,\n        num_images_per_prompt=num_images_per_prompt,\n    )["images"]\n\n    # create response\n    encoded_images = []\n    for image in generated_images:\n        buffered = BytesIO()\n        image.save(buffered, format="JPEG")\n        encoded_images.append(base64.b64encode(buffered.getvalue()).decode())\n\n    # create response\n    return {"generated_images": encoded_images}\n'})}),"\n",(0,t.jsx)(n.h2,{id:"prepare-llm-model",children:"Prepare LLM Model"}),"\n",(0,t.jsxs)(n.p,{children:["Fetch pre-trained, open source ",(0,t.jsx)(n.code,{children:"GPT-Neo"})," model, from Hugging Face."]}),"\n",(0,t.jsx)(n.h3,{id:"download-llm-from-huggingface",children:"Download LLM from HuggingFace"}),"\n",(0,t.jsx)(n.p,{children:"a few options"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["programmatically e.g. ",(0,t.jsx)(n.code,{children:"from transformers import AutoTokenizer, AutoModelForCausalLM"})]}),"\n",(0,t.jsxs)(n.li,{children:["git e.g. ",(0,t.jsx)(n.code,{children:"git lfs install && gti clone https://huggingface.co/distilgpt2"})]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["On the HuggingFace model ",(0,t.jsx)(n.a,{href:"https://huggingface.co/EleutherAI/gpt-neox-20b",children:"repo"})," page,"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"HuggingFace Repo",src:o(78418).A+"",width:"620",height:"223"})}),"\n",(0,t.jsxs)(n.p,{children:["I'm going with ",(0,t.jsx)(n.code,{children:"git"})," so need to install ",(0,t.jsx)(n.code,{children:"lfs"})," or"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# install\nsudo apt install git-lfs\n\n# enable\ngit lfs install\n"})}),"\n",(0,t.jsx)(n.p,{children:"pull GPT-NeoX-20b down"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"~/Repositories \u276f git clone https://huggingface.co/EleutherAI/gpt-neox-20b\nCloning into 'gpt-neox-20b'...\nremote: Enumerating objects: 125, done.\nremote: Counting objects: 100% (125/125), done.\nremote: Compressing objects: 100% (114/114), done.\nremote: Total 125 (delta 12), reused 121 (delta 10), pack-reused 0\nReceiving objects: 100% (125/125), 1.14 MiB | 1.69 MiB/s, done.\nResolving deltas: 100% (12/12), done.\n\nFiltering content: 100% (93/93), 76.91 GiB | 24.04 MiB/s, done.\n"})}),"\n",(0,t.jsx)(n.p,{children:"Damn, you can see there ^ the model is 76.91 GiB."}),"\n",(0,t.jsxs)(n.admonition,{title:"LLM Too Large?",type:"caution",children:[(0,t.jsx)(n.p,{children:"gpt-neox-20b i.e. 20 Billion parameter LLM included 46 pytorch bin files, which Claude explained looks like the LLM is sharded cos of the size, so I would have to try to either a) merge all bins or b) ensure the inference script was prepped to load all the shards first."}),(0,t.jsx)(n.p,{children:"I'm going to leave this and look for a smaller, simpler model to learn on first, and will come back to this later."})]}),"\n",(0,t.jsx)(n.p,{children:"Going with the GPT-125M LLM instead"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"~/Repositories \u276f git clone https://huggingface.co/EleutherAI/gpt-neo-125m\nCloning into 'gpt-neo-125m'...\nremote: Enumerating objects: 65, done.\nremote: Counting objects: 100% (7/7), done.\nremote: Compressing objects: 100% (7/7), done.\nremote: Total 65 (delta 2), reused 0 (delta 0), pack-reused 58\nUnpacking objects: 100% (65/65), 1.11 MiB | 2.23 MiB/s, done.\n\nFiltering content: 100% (4/4), 1.93 GiB | 21.55 MiB/s, done.\n"})}),"\n",(0,t.jsxs)(n.p,{children:["the file list of r ",(0,t.jsx)(n.code,{children:"gpt-neo-125m"})," looks more manageable and not sharded"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"~/R/gpt-neo-125m on main \u276f ll\ntotal 1.3G\n-rw-rw-r-- 1 rxhackk rxhackk 1007 Apr  2 19:01 config.json\n-rw-rw-r-- 1 rxhackk rxhackk 478M Apr  2 19:03 flax_model.msgpack\n-rw-rw-r-- 1 rxhackk rxhackk  119 Apr  2 19:01 generation_config.json\n-rw-rw-r-- 1 rxhackk rxhackk 446K Apr  2 19:01 merges.txt\n-rw-rw-r-- 1 rxhackk rxhackk 502M Apr  2 19:03 model.safetensors\n-rw-rw-r-- 1 rxhackk rxhackk 502M Apr  2 19:03 pytorch_model.bin\n-rw-rw-r-- 1 rxhackk rxhackk 4.1K Apr  2 19:01 README.md\n-rw-rw-r-- 1 rxhackk rxhackk 502M Apr  2 19:03 rust_model.ot\n-rw-rw-r-- 1 rxhackk rxhackk  357 Apr  2 19:01 special_tokens_map.json\n-rw-rw-r-- 1 rxhackk rxhackk  727 Apr  2 19:01 tokenizer_config.json\n-rw-rw-r-- 1 rxhackk rxhackk 2.1M Apr  2 19:01 tokenizer.json\n-rw-rw-r-- 1 rxhackk rxhackk 878K Apr  2 19:01 vocab.json\n"})}),"\n",(0,t.jsx)(n.h3,{id:"organise-model-artefacts",children:"Organise Model Artefacts"}),"\n",(0,t.jsx)(n.admonition,{title:"Claude 3",type:"tip",children:(0,t.jsxs)(n.p,{children:["I used ",(0,t.jsx)(n.a,{href:"https://claude.ai/chat/",children:"Claude 3 Opus"})," (paid) to generate the steps for me, and I just tweak any minor issues until it's working."]})}),"\n",(0,t.jsxs)(n.p,{children:["Create a new directory for your model (e.g., ",(0,t.jsx)(n.code,{children:"gpt-neo-125m-model"}),")."]}),"\n",(0,t.jsxs)(n.p,{children:["Copy the following files from the gpt-neo-125m model repository into the ",(0,t.jsx)(n.code,{children:"gpt-neo-125m-model"})," directory:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"config.json"}),": This file contains the model configuration parameters."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"pytorch_model.bin"}),": This file contains the pretrained model weights for PyTorch."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"tokenizer.json"}),": This file contains the tokenizer configuration."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"special_tokens_map.json"}),": This file contains the mappings for special tokens."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"vocab.json"}),": This file contains the vocabulary used by the tokenizer."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"merges.txt"}),": This file contains the byte-pair encoding (BPE) merges used by the tokenizer."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Create a ",(0,t.jsx)(n.code,{children:"requirements.txt"})," file in the ",(0,t.jsx)(n.code,{children:"gpt-neo-125m-model"})," directory and specify the dependencies required for your model. For example:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"transformers==4.12.0\ntorch==1.9.0\n"})}),"\n",(0,t.jsx)(n.p,{children:"Adjust the versions based on your specific requirements and compatibility."}),"\n",(0,t.jsxs)(n.p,{children:["Create a ",(0,t.jsx)(n.code,{children:"code/"})," directory in the ",(0,t.jsx)(n.code,{children:"gpt-neo-125m-model"})," directory and place any custom inference code or scripts you have written there. For example, you can create a ",(0,t.jsx)(n.code,{children:"inference.py"})," file with the following content:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import json\nimport torch\nfrom transformers import GPTNeoForCausalLM, GPT2Tokenizer\n\ndevice = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n\ndef model_fn(model_dir):\n    tokenizer = GPT2Tokenizer.from_pretrained(model_dir)\n    model = GPTNeoForCausalLM.from_pretrained(model_dir)\n    model.to(device)\n    model.eval()\n    return model, tokenizer\n\ndef input_fn(serialized_input_data, content_type):\n    if content_type == "application/json":\n        input_data = json.loads(serialized_input_data)\n        text = input_data["text"]\n        return text\n    raise Exception("Unsupported content type: {}".format(content_type))\n\ndef predict_fn(input_data, model_and_tokenizer):\n    model, tokenizer = model_and_tokenizer\n    input_ids = tokenizer.encode(input_data, return_tensors="pt").to(device)\n    output = model.generate(input_ids, max_length=100, num_return_sequences=1)\n    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n    return generated_text\n\ndef output_fn(prediction, accept):\n    if accept == "application/json":\n        return json.dumps({"generated_text": prediction}), accept\n    raise Exception("Unsupported accept type: {}".format(accept))\n'})}),"\n",(0,t.jsx)(n.p,{children:"Review code:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"model_fn(model_dir)"}),": This function is called by SageMaker to load the model. It takes the model_dir as input, which is the directory where the model files are stored. In this function, you load the tokenizer and the model, move the model to the appropriate device (CPU or GPU), and return the model and tokenizer."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"input_fn(serialized_input_data, content_type)"}),': This function is called by SageMaker to deserialize the input data. It takes the serialized input data and the content type as input. In this example, it expects the input data to be in JSON format with a "text" key containing the input text. You can modify this function to handle different input formats based on your requirements.']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"predict_fn(input_data, model_and_tokenizer)"}),": This function is called by SageMaker to perform the actual prediction. It takes the deserialized input data and the loaded model and tokenizer as input. In this example, it encodes the input text using the tokenizer, generates text using the model, and decodes the generated output."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"output_fn(prediction, accept)"}),": This function is called by SageMaker to serialize the prediction output. It takes the prediction and the desired output content type as input. In this example, it returns the generated text as a JSON response."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"package-deployment-model",children:"Package Deployment Model"}),"\n",(0,t.jsx)(n.p,{children:"Your directory should look like this:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"~/Repositories/SM-gpt-neo-125m/gpt-neo-125m-model \u276f ll\ntotal 332M\n-rw-rw-r-- 1 rxhackk rxhackk 1007 Apr  2 21:41 config.json\n-rw-rw-r-- 1 rxhackk rxhackk 1.2K Apr  2 22:02 inference.py\n-rw-rw-r-- 1 rxhackk rxhackk 446K Apr  2 21:41 merges.txt\n-rw-rw-r-- 1 rxhackk rxhackk 502M Apr  2 21:40 pytorch_model.bin\n-rw-rw-r-- 1 rxhackk rxhackk   34 Apr  2 21:55 requirements.txt\n-rw-rw-r-- 1 rxhackk rxhackk  357 Apr  2 21:41 special_tokens_map.json\n-rw-rw-r-- 1 rxhackk rxhackk  727 Apr  2 21:41 tokenizer_config.json\n-rw-rw-r-- 1 rxhackk rxhackk 878K Apr  2 21:41 vocab.json\n"})}),"\n",(0,t.jsx)(n.p,{children:"create a sub-dir, and organise your files like this:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"~/Repositories/SM-gpt-neo-125m/gpt-neo-125m-model \u276f tree                                                                                                          at \uf017 23:22:04\n.\n\u251c\u2500\u2500 code\n\u2502   \u251c\u2500\u2500 inference.py\n\u2502   \u2514\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 config.json\n\u251c\u2500\u2500 merges.txt\n\u251c\u2500\u2500 pytorch_model.bin\n\u251c\u2500\u2500 special_tokens_map.json\n\u251c\u2500\u2500 tokenizer_config.json\n\u2514\u2500\u2500 vocab.json\n\n1 directory, 8 files\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Compress the entire ",(0,t.jsx)(n.code,{children:"gpt-neo-125m-model"})," directory into a ",(0,t.jsx)(n.code,{children:"tar.gz"})," file using a command like:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# change into the model dir, where the `code/` subdir is\ncd gpt-neo-125m-model\ntar -czf gpt-neo-125m-model.tar.gz *\n"})}),"\n",(0,t.jsxs)(n.p,{children:["You now have a file ",(0,t.jsx)(n.code,{children:"gpt-neo-125m-model.tar.gz"})," that contains the necessary files for deploying the gpt-neo-125m model to SageMaker."]}),"\n",(0,t.jsx)(n.h2,{id:"deploy-model-to-sagemaker",children:"Deploy Model to SageMaker"}),"\n",(0,t.jsx)(n.p,{children:'Model is ready for upload, and then a SageMaker model using our "custom" LLM.'}),"\n",(0,t.jsx)(n.h3,{id:"upload-to-s3",children:"Upload to s3"}),"\n",(0,t.jsxs)(n.p,{children:["Upload the ",(0,t.jsx)(n.code,{children:"gpt-neo-125m-model.tar.gz"})," file to an S3 bucket."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"~/Repositories/SM-gpt-neo-125m \u276f aws s3 cp gpt-neo-125m-model.tar.gz s3://ra-aws-s3-lab                                                                     \u2718 252 at \uf017 22:11:03\nupload: ./gpt-neo-125m-model.tar.gz to s3://ra-aws-s3-lab/gpt-neo-125m-model.tar.gz\n"})}),"\n",(0,t.jsx)(n.h3,{id:"create-role",children:"Create Role"}),"\n",(0,t.jsxs)(n.p,{children:["cos I'm lazy, I got the ",(0,t.jsx)(n.code,{children:"aws cli"})," commands to do this from the terminal:"]}),"\n",(0,t.jsxs)(n.p,{children:["create ",(0,t.jsx)(n.code,{children:"assume-role-policy.json"})," to service-link our role:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'{\n  "Version": "2012-10-17",\n  "Statement": [\n    {\n      "Effect": "Allow",\n      "Principal": {\n        "Service": "sagemaker.amazonaws.com"\n      },\n      "Action": "sts:AssumeRole"\n    }\n  ]\n}\n'})}),"\n",(0,t.jsxs)(n.p,{children:["create role: ",(0,t.jsx)(n.code,{children:"aws iam create-role --role-name SageMakerExecutionRole --assume-role-policy-document file://assume-role-policy.json"})]}),"\n",(0,t.jsx)(n.p,{children:"output:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n    "Role": {\n        "Path": "/",\n        "RoleName": "SageMakerExecutionRole",\n        "RoleId": "AROA4EFKKWBQIC4MCQHWD",\n        "Arn": "arn:aws:iam::REDACTED:role/SageMakerExecutionRole",\n        "CreateDate": "2024-04-02T09:49:20+00:00",\n        "AssumeRolePolicyDocument": {\n            "Version": "2012-10-17",\n            "Statement": [\n                {\n                    "Effect": "Allow",\n                    "Principal": {\n                        "Service": "sagemaker.amazonaws.com"\n                    },\n                    "Action": "sts:AssumeRole"\n                }\n            ]\n        }\n    }\n}\n'})}),"\n",(0,t.jsxs)(n.p,{children:["attach the ",(0,t.jsx)(n.code,{children:"SageMakerFullAccess"})," policy: ",(0,t.jsx)(n.code,{children:"aws iam attach-role-policy --role-name SageMakerExecutionRole --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess"})]}),"\n",(0,t.jsx)(n.p,{children:"output: N/A"}),"\n",(0,t.jsxs)(n.p,{children:["get the ARN cos I need it for my deployment code: ",(0,t.jsx)(n.code,{children:"aws iam get-role --role-name SageMakerExecutionRole --query 'Role.Arn' --output text"})]}),"\n",(0,t.jsx)(n.p,{children:"output:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:"arn:aws:iam::REDACTED:role/SageMakerExecutionRole\n"})}),"\n",(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsxs)(n.p,{children:["These create role commands were successful, but when running my ",(0,t.jsx)(n.code,{children:"deploy.py"})," code, I ran into s3 permissions."]})}),"\n",(0,t.jsxs)(n.p,{children:["add ",(0,t.jsx)(n.code,{children:"AmazonS3ReadOnlyAccess"})," to get ",(0,t.jsx)(n.code,{children:"s3:GetObject"})," permissions for my role: ",(0,t.jsx)(n.code,{children:"aws iam attach-role-policy --role-name SageMakerExecutionRole --policy-arn arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"})]}),"\n",(0,t.jsx)(n.h3,{id:"create-endpoint",children:"Create Endpoint"}),"\n",(0,t.jsx)(n.p,{children:'Use the SageMaker Python SDK to create the "SageMaker Model".'}),"\n",(0,t.jsxs)(n.p,{children:["Now, according to Claude, you can run this code in a number of places, it doesn't matter, it will call the SageMaker API and stand up this model. You can call it locally (note: you need ",(0,t.jsx)(n.code,{children:"pip install sagemaker"}),"), or in a JupyterLab notebook, or AWS Cloud9."]}),"\n",(0,t.jsxs)(n.p,{children:["I'm going to try to call it locally, the following code I put into ",(0,t.jsx)(n.code,{children:"deploy.py"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from sagemaker.pytorch import PyTorchModel\n\nmodel_data = "s3://ra-aws-s3-lab/gpt-neo-125m-model.tar.gz"\nrole = "arn:aws:iam::REDACTED:role/SageMakerExecutionRole"\n\npytorch_model = PyTorchModel(\n    model_data=model_data,\n    role=role,\n    framework_version="1.9.0",\n    py_version="py38",\n    entry_point="inference.py"\n)\n\npredictor = pytorch_model.deploy(\n    instance_type="ml.m5.large",\n    initial_instance_count=1,\n)\n'})}),"\n",(0,t.jsx)(n.p,{children:"Success!"}),"\n",(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsxs)(n.p,{children:["there was a bunch of ",(0,t.jsx)(n.a,{href:"#troubleshooting",children:"troubleshooting"})," that happened before this stood up, and notes have been updated retroactively, but I've tried to capture what came out of the original process."]})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"~/Repositories/SM-gpt-neo-125m \u276f python3 deploy.py   at 23:43:51\nsagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/xdg-ubuntu/sagemaker/config.yaml\nsagemaker.config INFO - Not applying SDK defaults from location: /home/rxhackk/.config/sagemaker/config.yaml\n----------!%\n~/Repositories/SM-gpt-neo-125m \u276f   took 14m 1s at\n"})}),"\n",(0,t.jsx)(n.p,{children:"\u23f1\ufe0f 14mins to deploy."}),"\n",(0,t.jsx)(n.p,{children:"Check AWS Console"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Endpoint Success",src:o(25802).A+"",width:"1523",height:"239"})}),"\n",(0,t.jsx)(n.h2,{id:"test-the-endpoint",children:"Test the Endpoint"}),"\n",(0,t.jsx)(n.p,{children:"The moment of truth!"}),"\n",(0,t.jsx)(n.p,{children:'Now the custom model is up & running and our inference endpoint says "\u2705InService" we should be able to invoke it and get the inference script workingf:'}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'aws sagemaker-runtime invoke-endpoint \\\n  --endpoint-name my-endpoint \\\n  --cli-binary-format raw-in-base64-out \\\n  --body \'{"text": "Hello, LLM!"}\' \\\n  --content-type application/json output.txt\n'})}),"\n",(0,t.jsx)(n.p,{children:"I need my new endpoint name. I can get it via AWS Console, or use cli to grab it,:"}),"\n",(0,t.jsx)(n.p,{children:"To get ALL info:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.code,{children:"aws sagemaker list-endpoints"})}),"\n",(0,t.jsx)(n.p,{children:"To get just EndpointName in a table:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'aws sagemaker list-endpoints \\\n  --query "Endpoints[*].[EndpointName]" \\\n  --output table\n'})}),"\n",(0,t.jsx)(n.p,{children:"output"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"-----------------------------------------------------------\n|                      ListEndpoints                      |\n+---------------------------------------------------------+\n|  pytorch-inference-2024-04-03-10-52-17-710              |\n|  huggingface-pytorch-inference-2024-03-07-04-18-07-114  |\n|  DEMO-1709699475-cbc3-endpointx                         |\n|  SDXL-v2-1-RAMOS                                        |\n+---------------------------------------------------------+\n(END)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"invoke",children:"Invoke"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'aws sagemaker-runtime invoke-endpoint \\\n  --endpoint-name pytorch-inference-2024-04-03-10-52-17-710 \\\n  --cli-binary-format raw-in-base64-out \\\n  --body \'{"text": "Hi, LLM!"}\' \\\n  --content-type application/json output.txt\n'})}),"\n",(0,t.jsx)(n.p,{children:"response:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'{\n    "ContentType": "application/json",\n    "InvokedProductionVariant": "AllTraffic"\n}\n(END)\n'})}),"\n",(0,t.jsxs)(n.p,{children:["\u2705 We have a ",(0,t.jsx)(n.code,{children:"output.txt"})," file now from our LLM"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"\u276f cat output.txt   took  44s at  11:58:49\n{\"generated_text\": \"Hi, LLM!\\n\\nI'm a little confused about the word \\\"soul\\\" in the English language. I'm not sure what the word means in the English language, but I'm not sure what the word means in the English language. I'm not sure what the word means in the English language. I'm not sure what the word means in the English language. I'm not sure what the word means in the English language. I'm not sure what the word means in the\"}%\n"})}),"\n",(0,t.jsx)(n.p,{children:"Done."}),"\n",(0,t.jsx)(n.h2,{id:"bonus-re-deploy-updates-to-your-sagemaker-endpoint",children:"Bonus: Re-Deploy Updates to your SageMaker Endpoint"}),"\n",(0,t.jsxs)(n.p,{children:["I needed to update my ",(0,t.jsx)(n.code,{children:"inference.py"})," script to the following, to get better results:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'...\ndef predict_fn(input_data, model_and_tokenizer):\n    model, tokenizer = model_and_tokenizer\n    input_ids = tokenizer.encode(input_data, return_tensors="pt").to(device)\n    output = model.generate(\n        input_ids,\n        max_length=150,\n        num_return_sequences=1,\n        temperature=0.7,\n        top_k=50,\n        top_p=0.95\n    )\n    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n    return generated_text\n...\n'})}),"\n",(0,t.jsxs)(n.p,{children:["so the whole ",(0,t.jsx)(n.code,{children:"inference.py"})," now reads"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import json\nimport torch\nfrom transformers import GPTNeoForCausalLM, GPT2Tokenizer\n\ndevice = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n\ndef model_fn(model_dir):\n    tokenizer = GPT2Tokenizer.from_pretrained(model_dir)\n    model = GPTNeoForCausalLM.from_pretrained(model_dir)\n    model.to(device)\n    model.eval()\n    return model, tokenizer\n\ndef input_fn(serialized_input_data, content_type):\n    if content_type == "application/json":\n        input_data = json.loads(serialized_input_data)\n        text = input_data["text"]\n        return text\n    raise Exception("Unsupported content type: {}".format(content_type))\n\ndef predict_fn(input_data, model_and_tokenizer):\n    model, tokenizer = model_and_tokenizer\n    input_ids = tokenizer.encode(input_data, return_tensors="pt").to(device)\n    output = model.generate(\n        input_ids,\n        max_length=150,\n        num_return_sequences=1,\n        temperature=0.7,\n        top_k=50,\n        top_p=0.95\n    )\n    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n    return generated_text\n\ndef output_fn(prediction, accept):\n    if accept == "application/json":\n        return json.dumps({"generated_text": prediction}), accept\n    raise Exception("Unsupported accept type: {}".format(accept))\n\n'})}),"\n",(0,t.jsx)(n.p,{children:"retar it, re-upload to s3."}),"\n",(0,t.jsx)(n.h3,{id:"redeploy-sagemaker-model",children:"Redeploy SageMaker Model"}),"\n",(0,t.jsxs)(n.p,{children:["Here's the trick to de-deploying the same endpoint, you just need to add this to the existing ",(0,t.jsx)(n.code,{children:"deploy.py"})]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# original code\nfrom sagemaker.pytorch import PyTorchModel\n\nmodel_data = "s3://ra-aws-s3-lab/gpt-neo-125m-model.tar.gz"\nrole = "arn:aws:iam::REDACTED:role/SageMakerExecutionRole"\n\npytorch_model = PyTorchModel(\n    model_data=model_data,\n    role=role,\n    framework_version="1.9.0",\n    py_version="py38",\n    entry_point="inference.py"\n)\n\npredictor = pytorch_model.deploy(\n    instance_type="ml.m5.large",\n    initial_instance_count=1,\n    # add this oneline\n    update_endpoint=True\n)\n'})}),"\n",(0,t.jsxs)(n.p,{children:["Run re-deploy: ",(0,t.jsx)(n.code,{children:"python3 deploy.py"})]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"~/Repositories/SM-gpt-neo-125m \u276f python3 deploy.py                                           at  16:08:03\nsagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/xdg-ubuntu/sagemaker/config.yaml\nsagemaker.config INFO - Not applying SDK defaults from location: /home/rxhackk/.config/sagemaker/config.yaml\n-\n--------!%\n\uf31b \uf07c ~/Repositories/SM-gpt-neo-125m \u276f                                                       took  13m 40s\n"})}),"\n",(0,t.jsxs)(n.p,{children:["only took \u23f1\ufe0f ",(0,t.jsx)(n.strong,{children:"13m40s"})," this time."]}),"\n",(0,t.jsxs)(n.p,{children:["check endpoints: ",(0,t.jsx)(n.code,{children:'aws sagemaker list-endpoints --query "Endpoints[*].[EndpointName]" --output table'})]}),"\n",(0,t.jsx)(n.p,{children:"looks like its actually stop up a different endpoint:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"-----------------------------------------------------------\n|                      ListEndpoints                      |\n+---------------------------------------------------------+\n|  pytorch-inference-2024-04-04-03-16-43-017              |\n|  pytorch-inference-2024-04-03-10-52-17-710              |\n|  huggingface-pytorch-inference-2024-03-07-04-18-07-114  |\n|  DEMO-1709699475-cbc3-endpointx                         |\n|  SDXL-v2-1-RAMOS                                        |\n+---------------------------------------------------------+\n(END)\n"})}),"\n",(0,t.jsx)(n.p,{children:"picking the new endpoint in our invoke method"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'aws sagemaker-runtime invoke-endpoint \\\n  --endpoint-name pytorch-inference-2024-04-04-03-16-43-017 \\\n  --cli-binary-format raw-in-base64-out \\\n  --body \'{"text": "Hi! how are you?"}\' \\\n  --content-type application/json \\\n  output.txt\n'})}),"\n",(0,t.jsx)(n.p,{children:"new output, but probably just as sh! as the previous, think it needs a lot more config attention, but that's for another post."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cat output.txt\n{\"generated_text\": \"Hi! how are you?\\n\\nI'm back! I'm back! I'm back! I'm back! I'm back! I'm back! I'm back! I'm back! I'm back! I'm back! I'm back! I'm back! I'm back! I'm back! I'm back! I'm back! I'm back! I'm back! I'm back! I'm back! I'm back! I'm back! I'm back! I'm back! I'm back! I'm back! I'm back! I'm back! I'm back! I'm back! I'm back! I'm back! I'm back! I'm back! I'm back! I'm\"}%\n"})}),"\n",(0,t.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,t.jsx)(n.h3,{id:"python3-deploypy",children:"python3 deploy.py"}),"\n",(0,t.jsx)(n.p,{children:"Deploying the *.tar.gz model package from s3."}),"\n",(0,t.jsx)(n.p,{children:"Error:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateModel operation: Could not access model data at s3://ra-aws-s3-lab/gpt-neo-125m-model.tar.gz. Please ensure that the role "arn:aws:iam::REDACTED:role/SageMakerExecutionRole" exists and that its trust relationship policy allows the action "sts:AssumeRole" for the service principal "sagemaker.amazonaws.com". Also ensure that the role has "s3:GetObject" permissions and that the object is located in us-east-1. If your Model uses multiple models or uncompressed models, please ensure that the role has "s3:ListBucket" permission.\n'})}),"\n",(0,t.jsx)(n.p,{children:"I have double checked my IAM Role, according to each comment in the error message, and my Role is legit. I even tested by attaching Administrator access to the role, and it got the same error."}),"\n",(0,t.jsxs)(n.p,{children:["After reading ",(0,t.jsx)(n.a,{href:"https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/sagemaker.pytorch.html#pytorch-model",children:"SageMaker Python Docs"})," I saw my ",(0,t.jsx)(n.code,{children:"deploy.py"})," was missing an ",(0,t.jsx)(n.code,{children:"entry_point"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": I changed this"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'pytorch_model = PyTorchModel(\n    model_data=model_data,\n    role=role,\n    framework_version="1.9.0",\n    py_version="py38"\n)\n'})}),"\n",(0,t.jsx)(n.p,{children:"to this"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'pytorch_model = PyTorchModel(\n    model_data=model_data,\n    role=role,\n    framework_version="1.9.0",\n    py_version="py38",\n    entry_point="inference.py"\n)\n'})}),"\n",(0,t.jsx)(n.p,{children:"And the IAM Role error went away \ud83d\ude12 \u2705"}),"\n",(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsx)(n.p,{children:"I don't know why a missing parameter for pytorch model setup has to do with tripping on an IAM Role, maybe that's just python \ud83e\udd37."})}),"\n",(0,t.jsx)(n.h3,{id:"no-such-file-or-directory-inferencepy",children:"No such file or directory: 'inference.py'"}),"\n",(0,t.jsx)(n.p,{children:"error message:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/xdg-ubuntu/sagemaker/config.yaml\nsagemaker.config INFO - Not applying SDK defaults from location: /home/rxhackk/.config/sagemaker/config.yaml\nTraceback (most recent call last):\n  File "/home/rxhackk/Repositories/SM-gpt-neo-125m/deploy.py", line 14, in <module />\n    predictor = pytorch_model.deploy(\n  File "/home/rxhackk/.local/lib/python3.10/site-packages/sagemaker/model.py", line 1610, in deploy\n    self._create_sagemaker_model(\n  File "/home/rxhackk/.local/lib/python3.10/site-packages/sagemaker/model.py", line 865, in _create_sagemaker_model\n    container_def = self.prepare_container_def(\n  File "/home/rxhackk/.local/lib/python3.10/site-packages/sagemaker/pytorch/model.py", line 319, in prepare_container_def\n    self._upload_code(deploy_key_prefix, repack=self._is_mms_version())\n  File "/home/rxhackk/.local/lib/python3.10/site-packages/sagemaker/model.py", line 763, in _upload_code\n    utils.repack_model(\n  File "/home/rxhackk/.local/lib/python3.10/site-packages/sagemaker/utils.py", line 548, in repack_model\n    _create_or_update_code_dir(\n  File "/home/rxhackk/.local/lib/python3.10/site-packages/sagemaker/utils.py", line 609, in _create_or_update_code_dir\n    shutil.copy2(inference_script, code_dir)\n  File "/usr/lib/python3.10/shutil.py", line 434, in copy2\n    copyfile(src, dst, follow_symlinks=follow_symlinks)\n  File "/usr/lib/python3.10/shutil.py", line 254, in copyfile\n    with open(src, \'rb\') as fsrc:\nFileNotFoundError: [Errno 2] No such file or directory: \'inference.py\'\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": for me this was due to an incorrect directory strucuture for the model package I created in ",(0,t.jsx)(n.a,{href:"#package-deployment-model",children:"Package Deployment Model"})," (my original instructions, which I've fixed, but this was the step I borked this in)."]}),"\n",(0,t.jsxs)(n.p,{children:["Make sure you tar just the contents of the model package directory and not create a parent directory, i.e. when you untar or view the archive, you should see the ",(0,t.jsx)(n.code,{children:"code/"})," directory (where the inference code is) straight away."]}),"\n",(0,t.jsx)(n.h3,{id:"invalid-base64",children:"Invalid base64"}),"\n",(0,t.jsx)(n.p,{children:"Got this error:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'~/Repositories/SM-gpt-neo-125m \u276f aws sagemaker-runtime invoke-endpoint \\                                                                                    \u2718 254 at \uf017 11:52:01\n  --endpoint-name pytorch-inference-2024-04-03-10-52-17-710 \\\n  --body \'{"text": "Hi, LLM!"}\' \\\n  --content-type application/json output.txt\n\nInvalid base64: "{"text": "Hi, LLM!"}"\n'})}),"\n",(0,t.jsx)(n.p,{children:"I asked Claude3 and got these solutions:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'aws sagemaker-runtime invoke-endpoint \\          \u2718 255 at \uf017 11:52:39\n  --endpoint-name pytorch-inference-2024-04-03-10-52-17-710 \\\n  --body $(echo \'{"text": "Hi, LLM!"}\' | base64) \\\n  --content-type application/json \\\n  output.txt\n'})}),"\n",(0,t.jsx)(n.p,{children:"\u2705 works."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'aws sagemaker-runtime invoke-endpoint \\\n  --endpoint-name pytorch-inference-2024-04-03-10-52-17-710 \\\n  --cli-binary-format raw-in-base64-out \\\n  --body \'{"text": "Hi, LLM!"}\' \\\n  --content-type application/json \\\n  output.txt\n'})}),"\n",(0,t.jsx)(n.p,{children:"\u2705 works."})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},78418:(e,n,o)=>{o.d(n,{A:()=>r});const r=o.p+"assets/images/SageMakerDeploy-clonerepo-89d34f51dc1ddc9d25a20218414b336f.png"}}]);